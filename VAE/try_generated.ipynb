{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import chain\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "class RNN_VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    1. Hu, Zhiting, et al. \"Toward controlled generation of text.\" ICML. 2017.\n",
    "    2. Bowman, Samuel R., et al. \"Generating sentences from a continuous space.\" arXiv preprint arXiv:1511.06349 (2015).\n",
    "    3. Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vocab, h_dim, z_dim, p_word_dropout=0.3, unk_idx=0, pad_idx=1, start_idx=2, eos_idx=3, max_sent_len=15, pretrained_embeddings=None, freeze_embeddings=False, gpu=False):\n",
    "        super(RNN_VAE, self).__init__()\n",
    "\n",
    "        self.UNK_IDX = unk_idx\n",
    "        self.PAD_IDX = pad_idx\n",
    "        self.START_IDX = start_idx\n",
    "        self.EOS_IDX = eos_idx\n",
    "        self.MAX_SENT_LEN = max_sent_len\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.p_word_dropout = p_word_dropout\n",
    "\n",
    "        self.gpu = gpu\n",
    "\n",
    "        \"\"\"\n",
    "        Word embeddings layer\n",
    "        \"\"\"\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb_dim = h_dim\n",
    "            self.word_emb = nn.Embedding(n_vocab, h_dim, self.PAD_IDX)\n",
    "        else:\n",
    "            self.emb_dim = pretrained_embeddings.size(1)\n",
    "            self.word_emb = nn.Embedding(n_vocab, self.emb_dim, self.PAD_IDX)\n",
    "\n",
    "            # Set pretrained embeddings\n",
    "            self.word_emb.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "            if freeze_embeddings:\n",
    "                self.word_emb.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Encoder is GRU with FC layers connected to last hidden unit\n",
    "        \"\"\"\n",
    "        self.encoder = nn.GRU(self.emb_dim, h_dim)\n",
    "        self.q_mu = nn.Linear(h_dim, z_dim)\n",
    "        self.q_logvar = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder is GRU with `z` appended at its inputs\n",
    "        \"\"\"\n",
    "        self.decoder = nn.GRU(self.emb_dim+z_dim, z_dim, dropout=0.3)\n",
    "        self.decoder_fc = nn.Linear(z_dim, n_vocab)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Grouping the model's parameters: separating encoder, decoder\n",
    "        \"\"\"\n",
    "        self.encoder_params = chain(\n",
    "            self.encoder.parameters(), self.q_mu.parameters(),\n",
    "            self.q_logvar.parameters()\n",
    "        )\n",
    "\n",
    "        self.decoder_params = chain(\n",
    "            self.decoder.parameters(), self.decoder_fc.parameters()\n",
    "        )\n",
    "\n",
    "        self.vae_params = chain(\n",
    "            self.word_emb.parameters(), self.encoder_params, self.decoder_params\n",
    "        )\n",
    "        self.vae_params = filter(lambda p: p.requires_grad, self.vae_params)\n",
    "\n",
    "        \"\"\"\n",
    "        Use GPU if set\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward_encoder(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is batch of sentences: seq_len x mbsize\n",
    "        \"\"\"    \n",
    "        inputs = self.word_emb(inputs)\n",
    "        return self.forward_encoder_embed(inputs)\n",
    "\n",
    "    def forward_encoder_embed(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is embeddings of: seq_len x mbsize x emb_dim\n",
    "        \"\"\"\n",
    "        _, h = self.encoder(inputs, None)\n",
    "\n",
    "        # Forward to latent\n",
    "        h = h.view(-1, self.h_dim)\n",
    "        mu = self.q_mu(h)\n",
    "        logvar = self.q_logvar(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std*eps; eps ~ N(0, I)\n",
    "        \"\"\"\n",
    "        eps = Variable(torch.randn(self.z_dim))\n",
    "        eps = eps.cuda() if self.gpu else eps\n",
    "        return mu + torch.exp(logvar/2) * eps\n",
    "\n",
    "    def sample_z_prior(self, mbsize):\n",
    "        \"\"\"\n",
    "        Sample z ~ p(z) = N(0, I)\n",
    "        \"\"\"\n",
    "        z = Variable(torch.randn(mbsize, self.z_dim))\n",
    "        z = z.cuda() if self.gpu else z\n",
    "        return z\n",
    "\n",
    "    def forward_decoder(self, inputs, z):\n",
    "        \"\"\"\n",
    "        Inputs must be embeddings: seq_len x mbsize\n",
    "        \"\"\"\n",
    "        dec_inputs = self.word_dropout(inputs)\n",
    "\n",
    "        # Forward\n",
    "        seq_len = dec_inputs.size(0)\n",
    "\n",
    "        # 1 x mbsize x z_dim\n",
    "        init_h = z.unsqueeze(0)\n",
    "        inputs_emb = self.word_emb(dec_inputs)  # seq_len x mbsize x emb_dim\n",
    "        inputs_emb = torch.cat([inputs_emb, init_h.repeat(seq_len, 1, 1)], 2)\n",
    "\n",
    "        outputs, _ = self.decoder(inputs_emb, init_h)\n",
    "        seq_len, mbsize, _ = outputs.size()\n",
    "\n",
    "        outputs = outputs.view(seq_len*mbsize, -1)\n",
    "        y = self.decoder_fc(outputs)\n",
    "        y = y.view(seq_len, mbsize, self.n_vocab)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------\n",
    "        sentence: sequence of word indices.\n",
    "        use_c_prior: whether to sample `c` from prior or from `discriminator`.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        recon_loss: reconstruction loss of VAE.\n",
    "        kl_loss: KL-div loss of VAE.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        mbsize = sentence.size(1)\n",
    "\n",
    "        # sentence: '<start> I want to fly <eos>'\n",
    "        # enc_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_targets: 'I want to fly <eos> <pad>'\n",
    "        pad_words = Variable(torch.LongTensor([self.PAD_IDX])).repeat(1, mbsize)\n",
    "        pad_words = pad_words.cuda() if self.gpu else pad_words\n",
    "\n",
    "        enc_inputs = sentence\n",
    "        dec_inputs = sentence\n",
    "        dec_targets = torch.cat([sentence[1:], pad_words], dim=0)\n",
    "\n",
    "        # Encoder: sentence -> z\n",
    "        mu, logvar = self.forward_encoder(enc_inputs)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "\n",
    "        # Decoder: sentence -> y\n",
    "        y = self.forward_decoder(dec_inputs, z)       \n",
    "        \n",
    "        recon_loss = F.cross_entropy(\n",
    "            y.view(-1, self.n_vocab), dec_targets.view(-1), size_average=True\n",
    "        )\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar, 1))\n",
    "\n",
    "        return recon_loss, kl_loss,y\n",
    "\n",
    "    def generate_sentences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate sentences and corresponding z of (batch_size x max_sent_len)\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            z = self.sample_z_prior(1)\n",
    "            samples.append(self.sample_sentence(z, raw=True))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def sample_sentence(self, z, raw=False, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single sentence from p(x|z,c) according to given temperature.\n",
    "        `raw = True` means this returns sentence as in dataset which is useful\n",
    "        to train discriminator. `False` means that this will return list of\n",
    "        `word_idx` which is useful for evaluation.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "\n",
    "        z= z.view(1, 1, -1)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        if raw:\n",
    "            outputs.append(self.START_IDX)\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            emb = self.word_emb(word).view(1, 1, -1)\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "            output, h = self.decoder(emb, h)\n",
    "            y = self.decoder_fc(output).view(-1)\n",
    "            y = F.softmax(y/temp, dim=0)\n",
    "\n",
    "            idx = torch.multinomial(y,1)\n",
    "\n",
    "            word = Variable(torch.LongTensor([int(idx)]))\n",
    "            word = word.cuda() if self.gpu else word\n",
    "\n",
    "            idx = int(idx)\n",
    "\n",
    "            if not raw and idx == self.EOS_IDX:\n",
    "                break\n",
    "\n",
    "            outputs.append(idx)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        if raw:\n",
    "            outputs = Variable(torch.LongTensor(outputs)).unsqueeze(0)\n",
    "            return outputs.cuda() if self.gpu else outputs\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def generate_soft_embed(self, mbsize, temp=1):\n",
    "        \"\"\"\n",
    "        Generate soft embeddings of (mbsize x emb_dim) along with target z\n",
    "        and c for each row (mbsize x {z_dim, c_dim})\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        targets_z = []\n",
    "\n",
    "        for _ in range(mbsize):\n",
    "            z = self.sample_z_prior(1)\n",
    "\n",
    "            samples.append(self.sample_soft_embed(z, temp=1))\n",
    "            targets_z.append(z)\n",
    "\n",
    "        X_gen = torch.cat(samples, dim=0)\n",
    "        targets_z = torch.cat(targets_z, dim=0)\n",
    "\n",
    "        return X_gen, targets_z\n",
    "\n",
    "    def sample_soft_embed(self, z, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single soft embedded sentence from p(x|z,c) and temperature.\n",
    "        Soft embeddings are calculated as weighted average of word_emb\n",
    "        according to p(x|z,c).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        z = z.view(1, 1, -1)\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "        emb = self.word_emb(word).view(1, 1, -1)\n",
    "        emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = [self.word_emb(word).view(1, -1)]\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            output, h = self.decoder(emb, h)\n",
    "            o = self.decoder_fc(output).view(-1)\n",
    "\n",
    "            # Sample softmax with temperature\n",
    "            y = F.softmax(o / temp, dim=0)\n",
    "\n",
    "            # Take expectation of embedding given output prob -> soft embedding\n",
    "            # <y, w> = 1 x n_vocab * n_vocab x emb_dim\n",
    "            emb = y.unsqueeze(0) @ self.word_emb.weight\n",
    "            emb = emb.view(1, 1, -1)\n",
    "\n",
    "            # Save resulting soft embedding\n",
    "            outputs.append(emb.view(1, -1))\n",
    "\n",
    "            # Append with z and c for the next input\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        # 1 x 16 x emb_dim\n",
    "        outputs = torch.cat(outputs, dim=0).unsqueeze(0)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        return outputs.cuda() if self.gpu else outputs\n",
    "\n",
    "    def word_dropout(self, inputs):\n",
    "        \"\"\"\n",
    "        Do word dropout: with prob `p_word_dropout`, set the word to '<unk>'.\n",
    "        \"\"\"\n",
    "        if isinstance(inputs, Variable):\n",
    "            data = inputs.data.clone()\n",
    "        else:\n",
    "            data = inputs.clone()\n",
    "\n",
    "        # Sample masks: elems with val 1 will be set to <unk>\n",
    "        mask = torch.from_numpy(\n",
    "            np.random.binomial(1, p=self.p_word_dropout, size=tuple(data.size()))\n",
    "                     .astype('uint8')\n",
    "        )\n",
    "\n",
    "        if self.gpu:\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        # Set to <unk>\n",
    "        data[mask] = self.UNK_IDX\n",
    "\n",
    "        return Variable(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "sentence = inputs\n",
    "mbsize = sentence.size(1)\n",
    "\n",
    "# sentence: '<start> I want to fly <eos>'\n",
    "# enc_inputs: '<start> I want to fly <eos>'\n",
    "# dec_inputs: '<start> I want to fly <eos>'\n",
    "# dec_targets: 'I want to fly <eos> <pad>'\n",
    "pad_words = Variable(torch.LongTensor([self.PAD_IDX])).repeat(1, mbsize)\n",
    "pad_words = pad_words.cuda() if self.gpu else pad_words\n",
    "\n",
    "enc_inputs = sentence\n",
    "dec_inputs = sentence\n",
    "dec_targets = torch.cat([sentence[1:], pad_words], dim=0)\n",
    "\n",
    "# Encoder: sentence -> z\n",
    "mu, logvar = self.forward_encoder(enc_inputs)\n",
    "z = self.sample_z(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset all_sent_obftrain_less100.tsv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ctextgen.dataset import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "mb_size = 32\n",
    "z_dim = 20\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "lr_decay_every = 2000000\n",
    "n_iter = 20000\n",
    "log_interval = 1000\n",
    "z_dim = h_dim\n",
    "c_dim = 2\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "from torchtext import data\n",
    "\n",
    "#TEXT = data.Field(init_token='<start>', eos_token='<eos>', lower=True, tokenize='spacy',fix_length=20)\n",
    "TEXT = data.Field(init_token='<start>', eos_token='<eos>', lower=True, tokenize='spacy')\n",
    "LABEL = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "print(\"loading dataset all_sent_obftrain_less100.tsv...\")\n",
    "train  = data.TabularDataset.splits(\n",
    "        path='../sent/ori_gender_data/', \n",
    "        train='all_sent_obftrain_less100.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),('Label', LABEL)])[0]\n",
    "\n",
    "TEXT.build_vocab(train, max_size=30000, vectors=\"fasttext.en.300d\")\n",
    "#TEXT.build_vocab(train, max_size=10000)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "LABEL.vocab.stoi['1']=1\n",
    "LABEL.vocab.stoi['2']=2\n",
    "LABEL.vocab.stoi['3']=3\n",
    "LABEL.vocab.stoi['4']=4\n",
    "LABEL.vocab.stoi['5']=5\n",
    "\n",
    "\n",
    "model = RNN_VAE(\n",
    "    len(TEXT.vocab), h_dim, z_dim, p_word_dropout=0.3,max_sent_len=40,\n",
    "    pretrained_embeddings=TEXT.vocab.vectors, freeze_embeddings=False,\n",
    "    gpu=True\n",
    ")\n",
    "\n",
    "# fix parameters for word_emb and encoder of the model\n",
    "for p in model.word_emb.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.q_mu.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.q_logvar.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.vae_params = filter(lambda p: p.requires_grad, model.vae_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annealing for KL term\n",
    "kld_start_inc = 3000\n",
    "kld_weight = 0.01\n",
    "kld_max = 0.15\n",
    "kld_inc = (kld_max - kld_weight) / (n_iter - kld_start_inc)\n",
    "\n",
    "trainer = optim.Adam(model.vae_params, lr=lr)\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "dataset=train, batch_size=1,\n",
    "sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
    "\n",
    "\n",
    "#print(\"loading previous 200yelp_nofix_max40_predif.bin model\")\n",
    "model.load_state_dict(torch.load('models/{}.bin'.format('pre_yelp128_100')))\n",
    "    \n",
    "def save_model():\n",
    "    if not os.path.exists('models/'):\n",
    "        os.makedirs('models/')\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/{}.bin'.format('pre_yelp128_100_addgeneratedpredif'))   \n",
    "\n",
    "    \n",
    "pre_weight = 10\n",
    "\n",
    "#original_predif=[]\n",
    "#after_predif=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "inputs = batch.Text\n",
    "labels = batch.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(200000):\n",
    "    batch = next(iter(train_iter))\n",
    "    inputs = batch.Text\n",
    "    labels = batch.Label\n",
    "\n",
    "    \n",
    "    recon_loss, kl_loss, y = model.forward(inputs)\n",
    "    \n",
    "    after_sent = ' '.join([TEXT.vocab.itos[torch.argmax(x).item()] for x in y[:,0]])\n",
    "    m1 = predict_sentiment(after_sent,mrnn,mTEXT)\n",
    "    f1 = predict_sentiment(after_sent,frnn,fTEXT)\n",
    "    pre_dif_after = abs(f1-m1)\n",
    "    \n",
    "    loss = (recon_loss + kld_weight * kl_loss)*(pre_weight*pre_dif_after)\n",
    "    #print(\"pre_weight*pre_dif: \",pre_weight*pre_dif)\n",
    "\n",
    "    # Anneal kl_weight\n",
    "    if it > kld_start_inc and kld_weight < kld_max:\n",
    "        kld_weight += kld_inc\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm(model.vae_params, 5)\n",
    "    trainer.step()\n",
    "    trainer.zero_grad()\n",
    "\n",
    "    #if it % log_interval == 0:\n",
    "    if it%100==0:\n",
    "        original_sent = ' '.join([TEXT.vocab.itos[i] for i in inputs[:,0][1:]])\n",
    "        m = predict_sentiment(original_sent,mrnn,mTEXT)\n",
    "        f = predict_sentiment(original_sent,frnn,fTEXT)\n",
    "        pre_dif = abs(f-m)\n",
    "        print(original_sent)\n",
    "        print(\"mrnn original prediction: \",m)\n",
    "        print(\"frnn original prediction: \",f)\n",
    "        print(\"abs original dif: \",pre_dif)\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        print(after_sent)\n",
    "        print(\"mrnn after prediction: \",m1)\n",
    "        print(\"frnn after prediction: \",f1)\n",
    "        print(\"abs after dif: \",pre_dif_after)\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        z = model.sample_z_prior(1)\n",
    "        sample_idxs = model.sample_sentence(z)\n",
    "        sample_sent = ' '.join([TEXT.vocab.itos[i] for i in sample_idxs])\n",
    "\n",
    "        print('Iter-{}; Loss: {:.4f}; Recon: {:.4f}; KL: {:.4f}; Grad_norm: {:.4f};'\n",
    "              .format(it, loss.data[0], recon_loss.data[0], kl_loss.data[0], grad_norm))\n",
    "\n",
    "        print('Sample: \"{}\"'.format(sample_sent))\n",
    "        if sample_sent:\n",
    "            m = predict_sentiment(sample_sent,mrnn,mTEXT)\n",
    "            f = predict_sentiment(sample_sent,frnn,fTEXT)\n",
    "            print(\"mrnn sample prediction: \",m)\n",
    "            print(\"frnn sample prediction: \",f)\n",
    "            print(\"sample abs dif: \",abs(m-f))\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    # Anneal learning rate\n",
    "    new_lr = lr * (0.5 ** (it // lr_decay_every))\n",
    "    for param_group in trainer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "        \n",
    "    if it%1000==0:\n",
    "        print(\"saving model pre_yelp128_100_addgeneratedpredif.bin\")\n",
    "        print(\"\\n\")\n",
    "        save_model()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
