{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# pylint: disable=invalid-name, too-many-locals, too-many-arguments, no-member\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import texar as tx\n",
    "\n",
    "from ctrl_gen_model import CtrlGenModel\n",
    "\n",
    "\n",
    "config = importlib.import_module('config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from texar.modules import WordEmbedder, UnidirectionalRNNEncoder, \\\n",
    "        MLPTransformConnector, AttentionRNNDecoder, \\\n",
    "        GumbelSoftmaxEmbeddingHelper, Conv1DClassifier\n",
    "from texar.core import get_train_op\n",
    "from texar.utils import collect_trainable_variables, get_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "train_data = tx.data.MultiAlignedData(config.train_data)\n",
    "val_data = tx.data.MultiAlignedData(config.val_data)\n",
    "test_data = tx.data.MultiAlignedData(config.test_data)\n",
    "vocab = train_data.vocab(0)\n",
    "\n",
    "# Each training batch is used twice: once for updating the generator and\n",
    "# once for updating the discriminator. Feedable data iterator is used for\n",
    "# such case.\n",
    "iterator = tx.data.FeedableDataIterator(\n",
    "    {'train_g': train_data, 'train_d': train_data,\n",
    "     'val': val_data, 'test': test_data})\n",
    "batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guojy/texar/texar/modules/decoders/rnn_decoder_helpers.py:321: RelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:427: ExpRelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:429: Exp.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.exp) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bijectors/exp.py:73: PowerTransform.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.power_transform) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "gamma = tf.placeholder(dtype=tf.float32, shape=[], name='gamma')\n",
    "lambda_g = tf.placeholder(dtype=tf.float32, shape=[], name='lambda_g')\n",
    "model = CtrlGenModel(batch, vocab, gamma, lambda_g, config.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inside model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "inputs = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = WordEmbedder(\n",
    "    vocab_size=vocab.size,\n",
    "    hparams=self._hparams.embedder)\n",
    "encoder = UnidirectionalRNNEncoder(hparams=self._hparams.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9361"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<texar.hyperparams.HParams at 0x7f69ffe4a208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self._hparams.embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'word_embedder_2/word_embedder:0' shape=(9361, 100) dtype=float32_ref>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim_c': 200,\n",
       " 'dim_z': 500,\n",
       " 'embedder': {'dim': 100},\n",
       " 'encoder': {'rnn_cell': {'type': 'GRUCell',\n",
       "   'kwargs': {'num_units': 700},\n",
       "   'dropout': {'input_keep_prob': 0.5}}},\n",
       " 'decoder': {'rnn_cell': {'type': 'GRUCell',\n",
       "   'kwargs': {'num_units': 700},\n",
       "   'dropout': {'input_keep_prob': 0.5, 'output_keep_prob': 0.5}},\n",
       "  'attention': {'type': 'BahdanauAttention',\n",
       "   'kwargs': {'num_units': 700},\n",
       "   'attention_layer_size': 700},\n",
       "  'max_decoding_length_train': 21,\n",
       "  'max_decoding_length_infer': 20},\n",
       " 'classifier': {'kernel_size': [3, 4, 5],\n",
       "  'filters': 128,\n",
       "  'other_conv_kwargs': {'padding': 'same'},\n",
       "  'dropout_conv': [1],\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers': 0,\n",
       "  'num_classes': 1},\n",
       " 'opt': {'optimizer': {'type': 'AdamOptimizer',\n",
       "   'kwargs': {'learning_rate': 0.0005}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_ids for encoder, with BOS token removed\n",
    "enc_text_ids = inputs['text_ids'][:, 1:]\n",
    "enc_outputs, final_state = encoder(embedder(enc_text_ids),\n",
    "                                   sequence_length=inputs['length']-1)\n",
    "z = final_state[:, self._hparams.dim_c:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.initialize_dataset(sess)\n",
    "gamma_ = 1.\n",
    "lambda_g_ = 0.\n",
    "\n",
    "iterator.restart_dataset(sess, 'val')\n",
    "\n",
    "feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'val'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(enc_text_ids,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 700)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(enc_outputs,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 700)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(final_state,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 500)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(z,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._hparams.dim_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes label\n",
    "label_connector = MLPTransformConnector(self._hparams.dim_c)\n",
    "\n",
    "# Gets the sentence representation: h = (c, z)\n",
    "labels = tf.to_float(tf.reshape(inputs['labels'], [-1, 1]))\n",
    "c = label_connector(labels)\n",
    "c_ = label_connector(1 - labels)\n",
    "h = tf.concat([c, z], 1)\n",
    "h_ = tf.concat([c_, z], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(labels,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421],\n",
       "       ...,\n",
       "       [-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.14826657,  0.08401954, -0.0273709 , ..., -0.10147383,\n",
       "        -0.08362819, -0.12628421]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c_,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 700)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(final_state,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 700)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(h,feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7, 459,  29, ...,   8,   2,   0],\n",
       "       [ 25, 293,  30, ...,   0,   0,   0],\n",
       "       [612,  46,  11, ...,   4,   2,   0],\n",
       "       ...,\n",
       "       [ 21,  20,   6, ...,   0,   0,   0],\n",
       "       [ 14,  11,  71, ...,   0,   0,   0],\n",
       "       [  5,  17,  11, ...,   2,   0,   0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(model.samples['original'],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-force decoding and the auto-encoding loss for G\n",
    "decoder = AttentionRNNDecoder(\n",
    "    memory=enc_outputs,\n",
    "    memory_sequence_length=inputs['length']-1,\n",
    "    cell_input_fn=lambda inputs, attention: inputs,\n",
    "    vocab_size=vocab.size,\n",
    "    hparams=self._hparams.decoder)\n",
    "\n",
    "connector = MLPTransformConnector(decoder.state_size)\n",
    "\n",
    "g_outputs, _, _ = decoder(\n",
    "    initial_state=connector(h), inputs=inputs['text_ids'],\n",
    "    embedding=embedder, sequence_length=inputs['length']-1)\n",
    "\n",
    "loss_g_ae = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
    "    labels=inputs['text_ids'][:, 1:],\n",
    "    logits=g_outputs.logits,\n",
    "    sequence_length=inputs['length']-1,\n",
    "    average_across_timesteps=True,\n",
    "    sum_over_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sess.run(g_outputs,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9361,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.logits[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 9361)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 700)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 16)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 700)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gumbel-softmax decoding, used in training\n",
    "start_tokens = tf.ones_like(inputs['labels']) * vocab.bos_token_id\n",
    "end_token = vocab.eos_token_id\n",
    "gumbel_helper = GumbelSoftmaxEmbeddingHelper(\n",
    "    embedder.embedding, start_tokens, end_token, gamma)\n",
    "\n",
    "soft_outputs_, _, soft_length_, = decoder(\n",
    "    helper=gumbel_helper, initial_state=connector(h_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(start_tokens,feed_dict=feed_dict) #64个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sess.run(soft_outputs_,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.59797240e-06, 1.03159418e-05, 1.98395683e-05, ...,\n",
       "         3.64804087e-04, 2.03961054e-06, 1.05032559e-05],\n",
       "        [8.14804225e-05, 3.46020424e-05, 1.04083292e-05, ...,\n",
       "         2.92571121e-06, 9.70610836e-06, 1.37982797e-05],\n",
       "        [7.53640779e-06, 2.58242744e-05, 2.12993491e-05, ...,\n",
       "         8.60251475e-06, 2.15658383e-05, 3.90810164e-05],\n",
       "        ...,\n",
       "        [1.08514741e-05, 4.23368438e-05, 1.11591826e-05, ...,\n",
       "         8.01864007e-06, 2.85283977e-06, 8.00902490e-06],\n",
       "        [1.99825240e-06, 1.78916198e-05, 5.42382077e-06, ...,\n",
       "         3.63696631e-06, 5.31261821e-06, 1.70881194e-05],\n",
       "        [2.78724269e-06, 6.43636577e-06, 2.29557299e-05, ...,\n",
       "         3.91974891e-06, 8.76599552e-06, 2.24746582e-05]],\n",
       "\n",
       "       [[3.30221264e-05, 2.14687657e-06, 5.88184457e-06, ...,\n",
       "         2.00939326e-06, 3.24353323e-06, 1.01313447e-06],\n",
       "        [7.04760623e-06, 8.98932740e-06, 4.79178016e-06, ...,\n",
       "         6.74654366e-05, 6.75009233e-06, 1.10910260e-05],\n",
       "        [2.27829969e-05, 5.79276784e-05, 1.70600633e-05, ...,\n",
       "         4.68001554e-06, 3.03598654e-06, 2.24738869e-05],\n",
       "        ...,\n",
       "        [3.28690680e-06, 2.23929528e-05, 1.75764144e-05, ...,\n",
       "         4.01577163e-05, 1.61948847e-05, 2.45314677e-05],\n",
       "        [1.33543663e-05, 5.63131380e-06, 1.22374877e-05, ...,\n",
       "         1.25771912e-05, 3.84733767e-06, 2.02153624e-05],\n",
       "        [1.16874653e-05, 6.32684823e-05, 2.06103396e-05, ...,\n",
       "         5.90204800e-05, 4.16387957e-05, 2.89312902e-06]],\n",
       "\n",
       "       [[1.54302343e-05, 5.53755672e-06, 3.71805527e-06, ...,\n",
       "         9.98652740e-06, 4.90725070e-05, 3.15562306e-06],\n",
       "        [4.06417021e-05, 6.81117177e-04, 4.36461778e-06, ...,\n",
       "         1.62825690e-05, 2.29732628e-04, 5.14913918e-05],\n",
       "        [3.89295974e-06, 5.63207432e-05, 7.05188222e-06, ...,\n",
       "         8.01341866e-06, 4.03508930e-05, 4.33312653e-06],\n",
       "        ...,\n",
       "        [8.30023782e-06, 2.40043682e-05, 1.85639590e-06, ...,\n",
       "         5.48019761e-06, 3.47677451e-06, 6.39781683e-06],\n",
       "        [4.20520473e-06, 5.07976938e-06, 9.41408871e-06, ...,\n",
       "         3.42008229e-06, 6.30989507e-06, 2.71900732e-04],\n",
       "        [1.88254938e-03, 1.92800690e-05, 3.25680253e-06, ...,\n",
       "         2.75405046e-06, 3.18037288e-04, 3.04929654e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.08460326e-05, 1.49885147e-06, 6.64219169e-06, ...,\n",
       "         6.32924093e-06, 5.52404845e-06, 7.31725413e-06],\n",
       "        [7.88916714e-06, 4.63182596e-06, 1.53685542e-05, ...,\n",
       "         2.62600315e-05, 8.89256717e-06, 2.82112687e-05],\n",
       "        [2.29016114e-05, 3.41782761e-05, 4.35779248e-06, ...,\n",
       "         1.72041691e-05, 1.13232563e-05, 1.04653118e-05],\n",
       "        ...,\n",
       "        [9.01984731e-06, 8.62494107e-06, 5.44111390e-06, ...,\n",
       "         1.04401806e-05, 2.04409989e-05, 1.10312994e-05],\n",
       "        [1.85514655e-04, 3.44691798e-05, 2.41831549e-05, ...,\n",
       "         3.71324168e-05, 1.16581599e-04, 8.21265276e-05],\n",
       "        [5.38535242e-05, 2.63412239e-05, 6.89736726e-06, ...,\n",
       "         5.21257152e-06, 2.14709689e-05, 1.67767939e-05]],\n",
       "\n",
       "       [[1.62935976e-05, 3.17252270e-05, 4.54092806e-05, ...,\n",
       "         1.56458441e-06, 2.40112604e-05, 2.15272485e-05],\n",
       "        [1.39563372e-05, 1.06321131e-05, 3.55049747e-06, ...,\n",
       "         2.60120646e-06, 1.75286059e-05, 3.07106188e-06],\n",
       "        [2.32158939e-07, 9.87050015e-08, 7.11025478e-08, ...,\n",
       "         3.48194249e-07, 9.71880638e-08, 9.00389182e-07],\n",
       "        ...,\n",
       "        [4.58785462e-06, 1.84348428e-05, 1.17841875e-03, ...,\n",
       "         5.18497327e-05, 3.51436756e-06, 4.19257867e-06],\n",
       "        [1.88303766e-05, 1.35953605e-05, 1.00346966e-04, ...,\n",
       "         6.61568401e-06, 1.02036465e-05, 5.43673650e-06],\n",
       "        [1.68338578e-04, 1.10861829e-05, 1.55071230e-05, ...,\n",
       "         1.83575285e-05, 4.59191469e-05, 1.74542583e-05]],\n",
       "\n",
       "       [[1.12241723e-05, 1.59740630e-05, 1.82526273e-05, ...,\n",
       "         1.35688333e-05, 7.18549127e-04, 3.48025467e-04],\n",
       "        [2.51776015e-07, 6.79803065e-07, 1.45645026e-06, ...,\n",
       "         4.02764215e-07, 6.14099349e-07, 2.21509225e-07],\n",
       "        [6.43899693e-05, 1.05794636e-04, 3.33138769e-06, ...,\n",
       "         2.34012073e-03, 3.99929195e-05, 1.10254095e-05],\n",
       "        ...,\n",
       "        [2.89851437e-06, 6.13051634e-06, 1.18068310e-05, ...,\n",
       "         3.83532188e-06, 1.13131964e-05, 2.81963275e-06],\n",
       "        [3.24002576e-05, 8.30234421e-06, 7.95078086e-05, ...,\n",
       "         3.03969067e-03, 4.19604703e-06, 2.25150026e-04],\n",
       "        [4.16889379e-05, 2.40842564e-05, 9.40789596e-06, ...,\n",
       "         1.30118700e-04, 4.20881581e-04, 1.69343257e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 9361)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 9361)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 700)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 16)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 700)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20], dtype=int32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(soft_length_,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding, used in eval\n",
    "outputs_, _, length_ = decoder(\n",
    "    decoding_strategy='infer_greedy', initial_state=connector(h_),\n",
    "    embedding=embedder, start_tokens=start_tokens, end_token=end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sess.run(outputs_,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 9361)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], dtype=int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(length_,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates classifier\n",
    "classifier = Conv1DClassifier(hparams=self._hparams.classifier)\n",
    "clas_embedder = WordEmbedder(vocab_size=vocab.size,\n",
    "                             hparams=self._hparams.embedder)\n",
    "\n",
    "# Classification loss for the classifier\n",
    "clas_logits, clas_preds = classifier(\n",
    "    inputs=clas_embedder(ids=inputs['text_ids'][:, 1:]),\n",
    "    sequence_length=inputs['length']-1)\n",
    "loss_d_clas = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.to_float(inputs['labels']), logits=clas_logits)\n",
    "loss_d_clas = tf.reduce_mean(loss_d_clas)\n",
    "accu_d = tx.evals.accuracy(labels=inputs['labels'], preds=clas_preds)\n",
    "\n",
    "# Classification loss for the generator, based on soft samples\n",
    "soft_logits, soft_preds = classifier(\n",
    "    inputs=clas_embedder(soft_ids=soft_outputs_.sample_id),\n",
    "    sequence_length=soft_length_)\n",
    "loss_g_clas = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.to_float(1-inputs['labels']), logits=soft_logits)\n",
    "loss_g_clas = tf.reduce_mean(loss_g_clas)\n",
    "\n",
    "# Accuracy on soft samples, for training progress monitoring\n",
    "accu_g = tx.evals.accuracy(labels=1-inputs['labels'], preds=soft_preds)\n",
    "\n",
    "# Accuracy on greedy-decoded samples, for training progress monitoring\n",
    "_, gdy_preds = classifier(\n",
    "    inputs=clas_embedder(ids=outputs_.sample_id),\n",
    "    sequence_length=length_)\n",
    "accu_g_gdy = tx.evals.accuracy(\n",
    "    labels=1-inputs['labels'], preds=gdy_preds)\n",
    "\n",
    "# Aggregates losses\n",
    "loss_g = loss_g_ae + lambda_g * loss_g_clas\n",
    "loss_d = loss_d_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01741586, 0.03394279, 0.02264042, 0.01497537, 0.01940306,\n",
       "       0.01961852, 0.01133799, 0.02101284, 0.01516242, 0.01501904,\n",
       "       0.00919292, 0.01450225, 0.01402471, 0.01561443, 0.00870784,\n",
       "       0.01622221, 0.01207736, 0.01495792, 0.01874118, 0.00725839,\n",
       "       0.01232787, 0.01560966, 0.01846682, 0.01064307, 0.01111605,\n",
       "       0.01406671, 0.01103418, 0.01744304, 0.01826762, 0.00827709,\n",
       "       0.01784866, 0.0159074 , 0.02273712, 0.02134864, 0.01100616,\n",
       "       0.0170842 , 0.0185736 , 0.01049068, 0.01139423, 0.01997441,\n",
       "       0.0153355 , 0.01649436, 0.01238613, 0.01190534, 0.01948663,\n",
       "       0.0159913 , 0.00986294, 0.02123216, 0.0215549 , 0.01469933,\n",
       "       0.01515879, 0.01348502, 0.02196291, 0.01476233, 0.00659403,\n",
       "       0.02288788, 0.01546674, 0.02007548, 0.01291853, 0.01433617,\n",
       "       0.0152465 , 0.02132571, 0.02042782, 0.00671127], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(soft_logits,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(soft_preds,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accu_d,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(sess, gamma_, lambda_g_, epoch, verbose=True):\n",
    "    avg_meters_d = tx.utils.AverageRecorder(size=10)\n",
    "    avg_meters_g = tx.utils.AverageRecorder(size=10)\n",
    "\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            step += 1\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'train_d'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_\n",
    "            }\n",
    "\n",
    "            vals_d = sess.run(model.fetches_train_d, feed_dict=feed_dict)\n",
    "            avg_meters_d.add(vals_d)\n",
    "\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'train_g'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_\n",
    "            }\n",
    "            vals_g = sess.run(model.fetches_train_g, feed_dict=feed_dict)\n",
    "            avg_meters_g.add(vals_g)\n",
    "\n",
    "            if verbose and (step == 1 or step % config.display == 0):\n",
    "                print('step: {}, {}'.format(step, avg_meters_d.to_str(4)))\n",
    "                print('step: {}, {}'.format(step, avg_meters_g.to_str(4)))\n",
    "\n",
    "            if verbose and step % config.display_eval == 0:\n",
    "                iterator.restart_dataset(sess, 'val')\n",
    "                _eval_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('epoch: {}, {}'.format(epoch, avg_meters_d.to_str(4)))\n",
    "            print('epoch: {}, {}'.format(epoch, avg_meters_g.to_str(4)))\n",
    "            break\n",
    "\n",
    "def _eval_epoch(sess, gamma_, lambda_g_, epoch, val_or_test='val'):\n",
    "    avg_meters = tx.utils.AverageRecorder()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "            vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n",
    "\n",
    "            batch_size = vals.pop('batch_size')\n",
    "\n",
    "            # Computes BLEU\n",
    "            samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "            hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "            print(\"samples: \",hyps)\n",
    "\n",
    "            refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "            refs = np.expand_dims(refs, axis=1)\n",
    "            print(\"reference: \",refs)\n",
    "\n",
    "            bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "            vals['bleu'] = bleu\n",
    "\n",
    "            avg_meters.add(vals, weight=batch_size)\n",
    "\n",
    "            ###################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # Writes samples\n",
    "            tx.utils.write_paired_text(\n",
    "                refs.squeeze(), hyps,\n",
    "                os.path.join(config.sample_path, 'val.%d'%epoch),\n",
    "                append=True, mode='v')\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('{}: {}'.format(\n",
    "                val_or_test, avg_meters.to_str(precision=4)))\n",
    "            break\n",
    "\n",
    "    return avg_meters.avg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Table already initialized.\n\t [[Node: train_2/key_value_init_1 = InitializeTableV2[Tkey=DT_STRING, Tval=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](train_2/hash_table_1, train/key_value_init/values, train/key_value_init/keys)]]\n\nCaused by op 'train_2/key_value_init_1', defined at:\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-eb797ac752cd>\", line 4, in <module>\n    test_data = tx.data.MultiAlignedData(config.test_data)\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 133, in __init__\n    self._make_data()\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 483, in _make_data\n    self._vocab = self.make_vocab(self._hparams.datasets)\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 280, in make_vocab\n    eos_token=eos_token)\n  File \"/home/guojy/texar/texar/data/vocabulary.py\", line 108, in __init__\n    self.load(self._filename)\n  File \"/home/guojy/texar/texar/data/vocabulary.py\", line 163, in load\n    unk_token_idx)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 279, in __init__\n    super(HashTable, self).__init__(table_ref, default_value, initializer)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 171, in __init__\n    self._init = initializer.initialize(self)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 374, in initialize\n    table.table_ref, self._keys, self._values, name=scope)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 386, in initialize_table_v2\n    values=values, name=name)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Table already initialized.\n\t [[Node: train_2/key_value_init_1 = InitializeTableV2[Tkey=DT_STRING, Tval=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](train_2/hash_table_1, train/key_value_init/values, train/key_value_init/keys)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Table already initialized.\n\t [[Node: train_2/key_value_init_1 = InitializeTableV2[Tkey=DT_STRING, Tval=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](train_2/hash_table_1, train/key_value_init/values, train/key_value_init/keys)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-847518bcfcdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Table already initialized.\n\t [[Node: train_2/key_value_init_1 = InitializeTableV2[Tkey=DT_STRING, Tval=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](train_2/hash_table_1, train/key_value_init/values, train/key_value_init/keys)]]\n\nCaused by op 'train_2/key_value_init_1', defined at:\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-eb797ac752cd>\", line 4, in <module>\n    test_data = tx.data.MultiAlignedData(config.test_data)\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 133, in __init__\n    self._make_data()\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 483, in _make_data\n    self._vocab = self.make_vocab(self._hparams.datasets)\n  File \"/home/guojy/texar/texar/data/data/multi_aligned_data.py\", line 280, in make_vocab\n    eos_token=eos_token)\n  File \"/home/guojy/texar/texar/data/vocabulary.py\", line 108, in __init__\n    self.load(self._filename)\n  File \"/home/guojy/texar/texar/data/vocabulary.py\", line 163, in load\n    unk_token_idx)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 279, in __init__\n    super(HashTable, self).__init__(table_ref, default_value, initializer)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 171, in __init__\n    self._init = initializer.initialize(self)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 374, in initialize\n    table.table_ref, self._keys, self._values, name=scope)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 386, in initialize_table_v2\n    values=values, name=name)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Table already initialized.\n\t [[Node: train_2/key_value_init_1 = InitializeTableV2[Tkey=DT_STRING, Tval=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](train_2/hash_table_1, train/key_value_init/values, train/key_value_init/keys)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save2/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, 'save2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name_list = [v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_embedder/word_embedder:0',\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel:0',\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/gates/bias:0',\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel:0',\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias:0',\n",
       " 'mlp_connector/dense/kernel:0',\n",
       " 'mlp_connector/dense/bias:0',\n",
       " 'attention_rnn_decoder/memory_layer/kernel:0',\n",
       " 'mlp_connector_1/dense/kernel:0',\n",
       " 'mlp_connector_1/dense/bias:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v:0',\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel:0',\n",
       " 'attention_rnn_decoder/decoder/dense/kernel:0',\n",
       " 'attention_rnn_decoder/decoder/dense/bias:0',\n",
       " 'word_embedder_1/word_embedder:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/kernel:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/bias:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/kernel:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/bias:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/kernel:0',\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/bias:0',\n",
       " 'conv1d_classifier/conv_encoder/logit_layer/kernel:0',\n",
       " 'word_embedder_2/word_embedder:0',\n",
       " 'unidirectional_rnn_encoder_1/rnn/gru_cell/gates/kernel:0',\n",
       " 'unidirectional_rnn_encoder_1/rnn/gru_cell/gates/bias:0',\n",
       " 'unidirectional_rnn_encoder_1/rnn/gru_cell/candidate/kernel:0',\n",
       " 'unidirectional_rnn_encoder_1/rnn/gru_cell/candidate/bias:0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "reader = pywrap_tensorflow.NewCheckpointReader('save2/model.ckpt')\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias': [700],\n",
       " 'mlp_connector_1/dense/kernel': [700, 700],\n",
       " 'mlp_connector/dense/kernel': [1, 200],\n",
       " 'conv1d_classifier/conv_encoder/logit_layer/kernel': [384, 1],\n",
       " 'mlp_connector/dense/bias': [200],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/kernel': [4,\n",
       "  100,\n",
       "  128],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/kernel': [3,\n",
       "  100,\n",
       "  128],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/bias': [128],\n",
       " 'attention_rnn_decoder/memory_layer/kernel': [700, 700],\n",
       " 'attention_rnn_decoder/decoder/dense/bias': [9361],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel': [800,\n",
       "  1400],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias': [1400],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel': [800,\n",
       "  700],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel': [1400,\n",
       "  700],\n",
       " 'OptimizeLoss_2/word_embedder_1/word_embedder/Adam_1': [9361, 100],\n",
       " 'OptimizeLoss_2/learning_rate': [],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/logit_layer/kernel/Adam_1': [384,\n",
       "  1],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/logit_layer/kernel/Adam': [384,\n",
       "  1],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/kernel/Adam_1': [5,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/kernel/Adam': [5,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/kernel/Adam_1': [4,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss_2/beta2_power': [],\n",
       " 'OptimizeLoss_1/word_embedder/word_embedder/Adam': [9361, 100],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel/Adam_1': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel/Adam_1': [800,\n",
       "  700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/kernel/Adam_1': [3,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias/Adam_1': [700],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel/Adam': [800,\n",
       "  700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam': [700,\n",
       "  700],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias/Adam': [700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/bias/Adam_1': [128],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/gates/bias/Adam': [1400],\n",
       " 'OptimizeLoss/mlp_connector/dense/bias/Adam_1': [200],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel': [700,\n",
       "  700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/kernel/Adam': [4,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam': [700],\n",
       " 'OptimizeLoss/mlp_connector/dense/kernel/Adam': [1, 200],\n",
       " 'OptimizeLoss/mlp_connector/dense/kernel/Adam_1': [1, 200],\n",
       " 'OptimizeLoss/beta2_power': [],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/dense/kernel/Adam': [700, 9361],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1': [700,\n",
       "  700],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel/Adam': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss/mlp_connector_1/dense/bias/Adam': [700],\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel': [800, 700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/memory_layer/kernel/Adam': [700, 700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam': [700,\n",
       "  700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1': [700],\n",
       " 'attention_rnn_decoder/decoder/dense/kernel': [700, 9361],\n",
       " 'OptimizeLoss/mlp_connector_1/dense/kernel/Adam': [700, 700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel/Adam_1': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel/Adam_1': [800,\n",
       "  700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel/Adam': [1400,\n",
       "  700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/dense/kernel/Adam': [700, 9361],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1': [700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/bias/Adam_1': [128],\n",
       " 'OptimizeLoss/beta1_power': [],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias/Adam': [700],\n",
       " 'OptimizeLoss_1/mlp_connector/dense/bias/Adam': [200],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel/Adam': [1400,\n",
       "  700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias/Adam_1': [700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1': [1400,\n",
       "  700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias/Adam_1': [1400],\n",
       " 'OptimizeLoss/mlp_connector_1/dense/kernel/Adam_1': [700, 700],\n",
       " 'OptimizeLoss/mlp_connector_1/dense/bias/Adam_1': [700],\n",
       " 'OptimizeLoss_1/beta1_power': [],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/bias': [128],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel/Adam': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/memory_layer/kernel/Adam_1': [700, 700],\n",
       " 'OptimizeLoss/learning_rate': [],\n",
       " 'OptimizeLoss/attention_rnn_decoder/memory_layer/kernel/Adam_1': [700, 700],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel/Adam': [800,\n",
       "  700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/dense/bias/Adam': [9361],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/dense/bias/Adam_1': [9361],\n",
       " 'OptimizeLoss/word_embedder/word_embedder/Adam': [9361, 100],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel/Adam_1': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss_1/mlp_connector/dense/bias/Adam_1': [200],\n",
       " 'OptimizeLoss_1/mlp_connector_1/dense/kernel/Adam': [700, 700],\n",
       " 'OptimizeLoss_2/word_embedder_1/word_embedder/Adam': [9361, 100],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/dense/kernel/Adam_1': [700, 9361],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel/Adam': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/bias/Adam': [128],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/dense/kernel/Adam_1': [700,\n",
       "  9361],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/bias/Adam': [128],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias/Adam': [700],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel/Adam': [800,\n",
       "  700],\n",
       " 'OptimizeLoss/word_embedder/word_embedder/Adam_1': [9361, 100],\n",
       " 'word_embedder/word_embedder': [9361, 100],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_1/conv_1_1/bias': [128],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1': [1400,\n",
       "  700],\n",
       " 'word_embedder_1/word_embedder': [9361, 100],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam': [700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1': [700,\n",
       "  700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/bias/Adam_1': [128],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/candidate/bias/Adam_1': [700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias/Adam': [700],\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/gates/bias': [1400],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias/Adam_1': [700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel/Adam_1': [800,\n",
       "  700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias/Adam': [1400],\n",
       " 'OptimizeLoss/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias/Adam': [1400],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel/Adam': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/bias/Adam_1': [1400],\n",
       " 'unidirectional_rnn_encoder/rnn/gru_cell/gates/kernel': [800, 1400],\n",
       " 'OptimizeLoss_2/beta1_power': [],\n",
       " 'OptimizeLoss/mlp_connector/dense/bias/Adam': [200],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/gates/kernel/Adam_1': [800,\n",
       "  1400],\n",
       " 'OptimizeLoss_1/learning_rate': [],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/dense/bias/Adam': [9361],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/dense/bias/Adam_1': [9361],\n",
       " 'conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/kernel': [5,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss/unidirectional_rnn_encoder/rnn/gru_cell/gates/bias/Adam_1': [1400],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/memory_layer/kernel/Adam': [700, 700],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/bahdanau_attention/attention_v': [700],\n",
       " 'OptimizeLoss_1/attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/kernel/Adam': [800,\n",
       "  700],\n",
       " 'OptimizeLoss_1/beta2_power': [],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer/conv_1/kernel/Adam': [3,\n",
       "  100,\n",
       "  128],\n",
       " 'OptimizeLoss_1/mlp_connector/dense/kernel/Adam': [1, 200],\n",
       " 'mlp_connector_1/dense/bias': [700],\n",
       " 'attention_rnn_decoder/decoder/attention_wrapper/gru_cell/candidate/bias': [700],\n",
       " 'OptimizeLoss_1/mlp_connector/dense/kernel/Adam_1': [1, 200],\n",
       " 'OptimizeLoss_1/mlp_connector_1/dense/bias/Adam': [700],\n",
       " 'OptimizeLoss_1/word_embedder/word_embedder/Adam_1': [9361, 100],\n",
       " 'OptimizeLoss_1/mlp_connector_1/dense/bias/Adam_1': [700],\n",
       " 'OptimizeLoss_2/conv1d_classifier/conv_encoder/conv_pool_1/sequential_layer_2/conv_1_2/bias/Adam': [128],\n",
       " 'OptimizeLoss_1/mlp_connector_1/dense/kernel/Adam_1': [700, 700],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/candidate/kernel/Adam_1': [800,\n",
       "  700],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/gates/bias/Adam': [1400],\n",
       " 'OptimizeLoss_1/unidirectional_rnn_encoder/rnn/gru_cell/gates/bias/Adam_1': [1400]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_to_shape_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look up sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  ['lots of nice home restaurant and etc ago !'\n",
      " 'service was fast , friendly and professional .'\n",
      " 'customer service was awesome , the food was fantastic .'\n",
      " \"the room was fine , but i would n't sing about it .\" ')'\n",
      " 'the chicken screen was very tasteless and juicy .'\n",
      " 'the staff was excellent and efficient .'\n",
      " \"no server , i 'll just closed up the scallop .\"\n",
      " 'the staff is always friendly and willing to please .'\n",
      " 'they lost our saturdays and our orders !'\n",
      " 'love this place and we will go back !'\n",
      " \"it 's still the best dressing food in phoenix !\"\n",
      " \"does n't make it fun .\" 'the bathrooms was delicious .'\n",
      " 'customer service is horrible .' 'incredible !'\n",
      " 'also ordered the presentation and turkey techs which was very tasty .'\n",
      " 'the stay at the personal bottle made it a little better .'\n",
      " 'just enough people but small talented atmosphere .'\n",
      " 'the locations were ok , chicken was modern sing to over eating it .'\n",
      " 'food was delicious .' 'if your fries are hot , self yourself worked .'\n",
      " 'the excuses chili enchiladas were amazing .'\n",
      " 'the desserts are great , they have a good wine selection and store service .'\n",
      " 'waste their name is rocks !'\n",
      " 'they make their own washer which is delicious .'\n",
      " 'but our service was the worst i have ever had .'\n",
      " 'i where all my combo there and recommend them to all my wife .'\n",
      " 'i recommend !' 'my husband and i went for lunch on a saturday .'\n",
      " 'great sandwiches .' 'very good quality !' 'had a great breakfast .'\n",
      " 'fast , fries , and a beer for $ _num_ every discount .' 'best ever !'\n",
      " 'great service overall .'\n",
      " 'amazing , friendly staff and an incredible choice of wine !'\n",
      " 'its very small but very efficient .' 'the staff was great .'\n",
      " 'i will not be going back .'\n",
      " \"even for family style sale , i think it 's thinking anyway .\"\n",
      " 'you know , i used to really like this place .'\n",
      " 'very nice staff , great wine selection , and incredible food .'\n",
      " 'a couple of drinks to start , really expensive .'\n",
      " 'a restaurant that has always desired on their answering servers is getting a fallen !'\n",
      " 'this place is a product bagels with product service .'\n",
      " 'i love this place .' 'the price is perfect for what you get !'\n",
      " 'she was so polite and cold .' \"the server was n't bought .\"\n",
      " 'long ca steak we were penny a birthday .' 'i love to eat .'\n",
      " \"is n't this medical to be `` fast food '' ?\"\n",
      " 'oh my goodness ... it is a total touch !' 'always a checking !'\n",
      " 'lo burrito <UNK> at both .'\n",
      " \"this place was great , i love the biggest that it 's family owned !\"\n",
      " 'good food .' 'so if your not taken sense your probably over sing .'\n",
      " 'crap breakfast !'\n",
      " \"it 's an awesome , family run business in a cool old parking .\"\n",
      " '*the is a simple professional and he knows his subway .'\n",
      " 'not much flavor , just a beautifully of chicago .' 'then it went back .']\n",
      "reference:  [['lots of nice home decor and gift ideas !']\n",
      " ['service was fast , friendly and professional .']\n",
      " ['customer service was awesome , the food was fantastic .']\n",
      " [\"the room was fine , but i would n't rave about it .\"]\n",
      " [')']\n",
      " ['the chicken kabob was very tender and juicy .']\n",
      " ['the staff was excellent and efficient .']\n",
      " [\"no problem , i 'll just turn up the heater .\"]\n",
      " ['the staff is always friendly and eager to please .']\n",
      " ['they remember our names and our orders !']\n",
      " ['love this place and we will go back !']\n",
      " [\"it 's still the best greek food in phoenix !\"]\n",
      " [\"does n't make it fun .\"]\n",
      " ['the hummus was delicious .']\n",
      " ['customer service is horrible .']\n",
      " ['garbage !']\n",
      " ['also ordered the hummus and eggplant dip which was very tasty .']\n",
      " ['the stay at the holiday inn made it a little better .']\n",
      " ['just enough people but small intimate atmosphere .']\n",
      " ['the vegetables were ok , chicken was overcooked according to person eating it .']\n",
      " ['food was delicious .']\n",
      " ['if your fries are hot , consider yourself lucky .']\n",
      " ['the serrano chili enchiladas were amazing .']\n",
      " ['the desserts are great , they have a good wine selection and phenomenal service .']\n",
      " ['plus their name is funny !']\n",
      " ['they make their own brownies which is delicious .']\n",
      " ['but our service was the worst i have ever had .']\n",
      " ['i buy all my supplies there and recommend them to all my clients .']\n",
      " ['i recommend !']\n",
      " ['my husband and i went for lunch on a saturday .']\n",
      " ['great sandwiches .']\n",
      " ['very good quality !']\n",
      " ['had a great breakfast .']\n",
      " ['burger , fries , and a beer for $ _num_ every tuesday .']\n",
      " ['best ever !']\n",
      " ['great service overall .']\n",
      " ['amazing , friendly staff and an incredible choice of wine !']\n",
      " ['its very small but very efficient .']\n",
      " ['the staff was great .']\n",
      " ['i will not be going back .']\n",
      " [\"even for family style plates , i think it 's slightly overpriced .\"]\n",
      " ['you know , i used to really like this place .']\n",
      " ['very nice staff , great wine selection , and incredible food .']\n",
      " ['a couple of drinks to start , really expensive .']\n",
      " ['a restaurant that has always focused on their male servers is getting a facelift !']\n",
      " ['this place is a shit hole with shit service .']\n",
      " ['i love this place .']\n",
      " ['the price is perfect for what you get !']\n",
      " ['she was so polite and kind .']\n",
      " [\"the server was n't buying .\"]\n",
      " ['long story short we were celebrating a birthday .']\n",
      " ['i love to eat .']\n",
      " [\"is n't this supposed to be `` fast food '' ?\"]\n",
      " ['oh my goodness ... it is a total nightmare !']\n",
      " ['always a treat !']\n",
      " ['fiesta burrito <UNK> at both .']\n",
      " [\"this place was great , i love the fact that it 's family owned !\"]\n",
      " ['good food .']\n",
      " ['so if your not paying attention your probably over tipping .']\n",
      " ['yummy breakfast !']\n",
      " [\"it 's an awesome , family run business in a cool old building .\"]\n",
      " ['don is a true professional and he knows his craft .']\n",
      " ['not much flavor , just a hint of lemon .']\n",
      " ['then it went downhill .']]\n"
     ]
    }
   ],
   "source": [
    "iterator.initialize_dataset(sess)\n",
    "gamma_ = 1.\n",
    "lambda_g_ = 0.\n",
    "\n",
    "feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'val'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n",
    "\n",
    "batch_size = vals.pop('batch_size')\n",
    "\n",
    "# Computes BLEU\n",
    "samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "print(\"samples: \",hyps)\n",
    "\n",
    "refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "refs = np.expand_dims(refs, axis=1)\n",
    "print(\"reference: \",refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        if config.restore:\n",
    "            print('Restore from: {}'.format(config.restore))\n",
    "            saver.restore(sess, config.restore)\n",
    "\n",
    "        iterator.initialize_dataset(sess)\n",
    "\n",
    "        gamma_ = 1.\n",
    "        lambda_g_ = 0.\n",
    "        for epoch in range(1, config.max_nepochs+1):\n",
    "            if epoch > config.pretrain_nepochs:\n",
    "                # Anneals the gumbel-softmax temperature\n",
    "                gamma_ = max(0.001, gamma_ * config.gamma_decay)\n",
    "                lambda_g_ = config.lambda_g\n",
    "            print('gamma: {}, lambda_g: {}'.format(gamma_, lambda_g_))\n",
    "\n",
    "            # Train\n",
    "            iterator.restart_dataset(sess, ['train_g', 'train_d'])\n",
    "            _train_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "            # Val\n",
    "            iterator.restart_dataset(sess, 'val')\n",
    "            _eval_epoch(sess, gamma_, lambda_g_, epoch, 'val')\n",
    "\n",
    "            #saver.save(sess, os.path.join(config.checkpoint_path, 'ckpt'), epoch)\n",
    "            saver.save(sess, os.path.join('screensave', 'model'), epoch)\n",
    "\n",
    "            # Test\n",
    "            iterator.restart_dataset(sess, 'test')\n",
    "            _eval_epoch(sess, gamma_, lambda_g_, epoch, 'test')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main=_main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
