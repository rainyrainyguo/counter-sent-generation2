{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor epoch in range(1, 20):\\n    # Train\\n    iterator.restart_dataset(sess, ['train_g'])\\n    _train_epoch(sess, epoch)\\n  \\nsaver.save(sess,'RLsave/mydata_wholemodel.ckpt')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import texar as tx\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import texar as tx\n",
    "from texar.modules import WordEmbedder, UnidirectionalRNNEncoder, \\\n",
    "        MLPTransformConnector, AttentionRNNDecoder, \\\n",
    "        GumbelSoftmaxEmbeddingHelper, Conv1DClassifier\n",
    "from texar.core import get_train_op\n",
    "from texar.utils import collect_trainable_variables, get_batch_size\n",
    "\n",
    "\n",
    "class RLModel(object):\n",
    "    \"\"\"Control\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, vocab,hparams=None):\n",
    "        self._hparams = tx.HParams(hparams, None)\n",
    "        self._build_model(inputs, vocab)\n",
    "\n",
    "    def _build_model(self, inputs, vocab):\n",
    "        \"\"\"Builds the model.\n",
    "        \"\"\"\n",
    "        embedder = WordEmbedder(\n",
    "            vocab_size=vocab.size,\n",
    "            hparams=self._hparams.embedder)\n",
    "        encoder = UnidirectionalRNNEncoder(hparams=self._hparams.encoder)\n",
    "\n",
    "        # text_ids for encoder, with BOS token removed\n",
    "        enc_text_ids = inputs['text_ids'][:, 1:]\n",
    "        enc_outputs, final_state = encoder(embedder(enc_text_ids),\n",
    "                                           sequence_length=inputs['length']-1)\n",
    "\n",
    "        z = final_state\n",
    "        h = z\n",
    "\n",
    "        # Teacher-force decoding and the auto-encoding loss for G\n",
    "        decoder = AttentionRNNDecoder(\n",
    "            memory=enc_outputs,\n",
    "            memory_sequence_length=inputs['length']-1,\n",
    "            cell_input_fn=lambda inputs, attention: inputs,\n",
    "            vocab_size=vocab.size,\n",
    "            hparams=self._hparams.decoder)\n",
    "\n",
    "        connector = MLPTransformConnector(decoder.state_size)\n",
    "\n",
    "        g_outputs, _, _ = decoder(\n",
    "            initial_state=connector(h), inputs=inputs['text_ids'],\n",
    "            embedding=embedder, sequence_length=inputs['length']-1)\n",
    "\n",
    "        loss_g_ae = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
    "            labels=inputs['text_ids'][:, 1:],\n",
    "            logits=g_outputs.logits,\n",
    "            sequence_length=inputs['length']-1,\n",
    "            average_across_timesteps=True,\n",
    "            sum_over_timesteps=False)       \n",
    "        \n",
    "        # Greedy decoding, used in eval (and RL training)\n",
    "        start_tokens = tf.ones_like(inputs['labels']) * vocab.bos_token_id\n",
    "        end_token = vocab.eos_token_id\n",
    "        outputs, _, length = decoder(\n",
    "            decoding_strategy='infer_greedy', initial_state=connector(h),\n",
    "            embedding=embedder, start_tokens=start_tokens, end_token=end_token)\n",
    "        \n",
    "        ######################## RL training\n",
    "        loss_g_RL2 = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
    "        labels=outputs.sample_id,\n",
    "        logits=outputs.logits,\n",
    "        #sequence_length=tf.convert_to_tensor(outputs.sample_id.shape[1]),\n",
    "        sequence_length = 30, #好像，如果设置了sum_over_timesteps=True之后，sequence_length就没有影响了，因为反正也是求loss的sum\n",
    "        average_across_timesteps=False,\n",
    "        sum_over_timesteps=True)\n",
    "        \n",
    "        g_vars_RL2 = collect_trainable_variables([connector, decoder])       \n",
    "        train_op_g_RL2 = get_train_op(loss_g_RL2, g_vars_RL2, hparams=self._hparams.opt)\n",
    "        \n",
    "        self.train_g_RL2 = {\n",
    "            \"loss_g_RL2\":loss_g_RL2,\n",
    "            \"train_op_g_RL2\": train_op_g_RL2\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Creates optimizers\n",
    "        g_vars = collect_trainable_variables([embedder, encoder, connector, decoder])       \n",
    "        train_op_g_ae = get_train_op(loss_g_ae, g_vars, hparams=self._hparams.opt)\n",
    "        \n",
    "        # Interface tensors\n",
    "        self.train_g = {\n",
    "            \"loss_g_ae\":loss_g_ae,\n",
    "            \"train_op_g_ae\": train_op_g_ae\n",
    "        }\n",
    "        self.samples = {\n",
    "            \"batch_size\": get_batch_size(inputs['text_ids']),\n",
    "            \"original\": inputs['text_ids'][:, 1:],\n",
    "            \"transferred\": outputs.sample_id #outputs 是infer_greedy的结果\n",
    "        }       \n",
    "        \n",
    "        \n",
    "config = importlib.import_module('RLconfig')\n",
    "\n",
    "# Data\n",
    "train_data = tx.data.MultiAlignedData(config.train_data)\n",
    "val_data = tx.data.MultiAlignedData(config.val_data)\n",
    "test_data = tx.data.MultiAlignedData(config.test_data)\n",
    "vocab = train_data.vocab(0)\n",
    "\n",
    "# Each training batch is used twice: once for updating the generator and\n",
    "# once for updating the discriminator. Feedable data iterator is used for\n",
    "# such case.\n",
    "iterator = tx.data.FeedableDataIterator({'train_g': train_data,'val': val_data, 'test': test_data})\n",
    "batch = iterator.get_next()\n",
    "\n",
    "model = RLModel(batch, vocab, config.model)\n",
    "\n",
    "def _train_epoch(sess, epoch, verbose=True):\n",
    "    avg_meters_g = tx.utils.AverageRecorder(size=10)\n",
    "\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            step += 1\n",
    "            \n",
    "            feed_dict = {iterator.handle: iterator.get_handle(sess, 'train_g')}\n",
    "            vals_g = sess.run(model.train_g, feed_dict=feed_dict)\n",
    "            avg_meters_g.add(vals_g)\n",
    "\n",
    "            if verbose and (step == 1 or step % 5 == 0):\n",
    "                print('step: {}, {}'.format(step, avg_meters_g.to_str(4)))\n",
    "\n",
    "            '''\n",
    "            if verbose and step % 2 == 0:\n",
    "                iterator.restart_dataset(sess, 'val')\n",
    "                _eval_epoch(sess, epoch)\n",
    "            '''\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('epoch: {}, {}'.format(epoch, avg_meters_g.to_str(4)))\n",
    "            break\n",
    "            \n",
    "def _eval_epoch(sess, epoch, val_or_test='val'):\n",
    "    avg_meters = tx.utils.AverageRecorder()\n",
    "    while True:\n",
    "        try:\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "            vals = sess.run(model.samples, feed_dict=feed_dict)\n",
    "\n",
    "            batch_size = vals.pop('batch_size')\n",
    "\n",
    "            # Computes BLEU\n",
    "            samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "            hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "            print(\"samples: \",hyps)\n",
    "\n",
    "            refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "            refs = np.expand_dims(refs, axis=1)\n",
    "            print(\"reference: \",refs)\n",
    "\n",
    "            bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "            vals['bleu'] = bleu\n",
    "\n",
    "            avg_meters.add(vals, weight=batch_size)\n",
    "\n",
    "            ###################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # Writes samples\n",
    "            '''\n",
    "            tx.utils.write_paired_text(\n",
    "                refs.squeeze(), hyps,\n",
    "                os.path.join(config.sample_path, 'val.%d'%epoch),\n",
    "                append=True, mode='v')\n",
    "            '''\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('{}: {}'.format(\n",
    "                val_or_test, avg_meters.to_str(precision=4)))\n",
    "            break\n",
    "    return avg_meters.avg()\n",
    "\n",
    "def write_predif_AE(sess,val_or_test='test'):\n",
    "    iterator.initialize_dataset(sess)\n",
    "    sample_sents=[]\n",
    "    ref_sents=[]\n",
    "    i=1\n",
    "    while True:\n",
    "        print(\"batch: \",i)\n",
    "        i=i+1\n",
    "        try:\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "            vals = sess.run(model.samples, feed_dict=feed_dict)\n",
    "\n",
    "            batch_size = vals.pop('batch_size')\n",
    "\n",
    "            # Computes BLEU\n",
    "            samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "            hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "            #print(\"samples: \",hyps)\n",
    "\n",
    "            refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "            refs = np.expand_dims(refs, axis=1)\n",
    "            #print(\"reference: \",refs)\n",
    "            \n",
    "            sample_sents.extend(hyps.tolist())\n",
    "            ref_sents.extend(refs.tolist())\n",
    "            \n",
    "            #dif = np.abs(predict_sentiment(str(hyps[0]),frnn,fTEXT)-predict_sentiment(str(hyps[0]),mrnn,mTEXT))\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"all batches finished\")\n",
    "            break\n",
    "    return sample_sents,ref_sents\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "iterator.initialize_dataset(sess)\n",
    "\n",
    "'''\n",
    "inputs = batch\n",
    "self = model\n",
    "\n",
    "\"\"\"Builds the model.\n",
    "\"\"\"\n",
    "embedder = WordEmbedder(\n",
    "    vocab_size=vocab.size,\n",
    "    hparams=self._hparams.embedder)\n",
    "encoder = UnidirectionalRNNEncoder(hparams=self._hparams.encoder)\n",
    "\n",
    "# text_ids for encoder, with BOS token removed\n",
    "enc_text_ids = inputs['text_ids'][:, 1:]\n",
    "enc_outputs, final_state = encoder(embedder(enc_text_ids),\n",
    "                                   sequence_length=inputs['length']-1)\n",
    "#z = final_state[:, self._hparams.dim_c:]\n",
    "z = final_state\n",
    "h = z\n",
    "\n",
    "\n",
    "# Teacher-force decoding and the auto-encoding loss for G\n",
    "decoder = AttentionRNNDecoder(\n",
    "    memory=enc_outputs,\n",
    "    memory_sequence_length=inputs['length']-1,\n",
    "    cell_input_fn=lambda inputs, attention: inputs,\n",
    "    vocab_size=vocab.size,\n",
    "    hparams=self._hparams.decoder)\n",
    "\n",
    "connector = MLPTransformConnector(decoder.state_size)\n",
    "\n",
    "g_outputs, _, _ = decoder(\n",
    "    initial_state=connector(h), inputs=inputs['text_ids'],\n",
    "    embedding=embedder, sequence_length=inputs['length']-1)\n",
    "\n",
    "loss_g_ae = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
    "    labels=inputs['text_ids'][:, 1:],\n",
    "    logits=g_outputs.logits,\n",
    "    sequence_length=inputs['length']-1,\n",
    "    average_across_timesteps=True,\n",
    "    sum_over_timesteps=False)\n",
    "\n",
    "start_tokens = tf.ones_like(inputs['labels']) * vocab.bos_token_id\n",
    "end_token = vocab.eos_token_id\n",
    "outputs, _, length = decoder(\n",
    "decoding_strategy='infer_greedy', initial_state=connector(h),\n",
    "embedding=embedder, start_tokens=start_tokens, end_token=end_token)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "#batch_size=1的时候， 不需要reshape\n",
    "loss_g_RL2 = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
    "    labels=outputs.sample_id,\n",
    "    logits=outputs.logits,\n",
    "    #sequence_length=tf.convert_to_tensor(outputs.sample_id.shape[1]),\n",
    "    sequence_length = 30, #好像，如果设置了sum_over_timesteps=True之后，sequence_length就没有影响了，因为反正也是求loss的sum\n",
    "    average_across_timesteps=False,\n",
    "    sum_over_timesteps=True)\n",
    "\n",
    "\n",
    "\n",
    "feed_dict = {iterator.handle: iterator.get_handle(sess, 'train_g')} #train mode\n",
    "vals = sess.run({'a':model.samples,'b':outputs.logits}, feed_dict=feed_dict)\n",
    "\n",
    "batch_size = vals['a'].pop('batch_size')\n",
    "# Computes BLEU\n",
    "samples = tx.utils.dict_pop(vals['a'], list(model.samples.keys()))\n",
    "hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "refs = np.expand_dims(refs, axis=1)\n",
    "#dif = np.abs(predict_sentiment(str(hyps[0]),frnn,fTEXT)-predict_sentiment(str(hyps[0]),mrnn,mTEXT))\n",
    "\n",
    "\n",
    "dif = np.abs(len(hyps[0])/10)\n",
    "#dif = np.abs(predict_sentiment(str(hyps[0]),frnn,fTEXT)-predict_sentiment(str(hyps[0]),mrnn,mTEXT))\n",
    "\n",
    "if dif>1:\n",
    "    loss_g_RL = dif*loss_g_RL2\n",
    "else:\n",
    "    loss_g_RL = loss_g_RL2\n",
    "\n",
    "g_vars_RL = collect_trainable_variables([connector, decoder])       \n",
    "train_op_g_RL = get_train_op(loss_g_RL, g_vars_RL, hparams=self._hparams.opt)\n",
    "\n",
    "# Interface tensors\n",
    "model.train_g_RL = {\n",
    "    #'dif':dif,\n",
    "    'loss_g_RL2':loss_g_RL2,\n",
    "    \"loss_g_RL\":loss_g_RL,\n",
    "    \"train_op_g_RL\": train_op_g_RL,\n",
    "    \"original\": inputs['text_ids'][:, 1:],\n",
    "    \"transferred\": outputs.sample_id #outputs 是infer_greedy的结果\n",
    "}\n",
    "\n",
    "'''\n",
    "#after running thtough all the newly added variables, we can initialize the variables \n",
    "#(this will overwrite the previous loaded train_g_ae model)\n",
    "# 每次新加进去了变量之后都要initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "#sess.run(tf.tables_initializer())\n",
    "\n",
    "#saver.restore(sess,'RLsave/mydata_wholemodel.ckpt')\n",
    "\n",
    "'''\n",
    "for epoch in range(1, 20):\n",
    "    # Train\n",
    "    iterator.restart_dataset(sess, ['train_g'])\n",
    "    _train_epoch(sess, epoch)\n",
    "  \n",
    "saver.save(sess,'RLsave/mydata_wholemodel.ckpt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.save(sess,'RLsave/mydata_embedded_AEmodel1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from RLsave/mydata_embedded_AEmodel1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from RLsave/mydata_embedded_AEmodel1.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'RLsave/mydata_embedded_AEmodel1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from RLsave/mydata_embedded_AEmodel64.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'RLsave/mydata_embedded_AEmodel64.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  ['the double double is just the best best burger in the']\n",
      "reference:  [['the double double is just the best best burger in the universe .']]\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {\n",
    "    iterator.handle: iterator.get_handle(sess, 'val'),\n",
    "    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "}\n",
    "\n",
    "vals = sess.run(model.samples, feed_dict=feed_dict)\n",
    "\n",
    "batch_size = vals.pop('batch_size')\n",
    "\n",
    "# Computes BLEU\n",
    "samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "print(\"samples: \",hyps)\n",
    "\n",
    "refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "refs = np.expand_dims(refs, axis=1)\n",
    "print(\"reference: \",refs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = np.abs(predict_sentiment(str(hyps[0]),frnn,fTEXT)-predict_sentiment(str(hyps[0]),mrnn,mTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_g_RL2': 5.114773, 'train_op_g_RL2': 5.114773}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(model.train_g_RL2,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    iterator.handle: iterator.get_handle(sess, 'val'),\n",
    "    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sess.run(model.samples,feed_dict=feed_dict)['transferred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': <tf.Tensor 'strided_slice_2:0' shape=() dtype=int32>,\n",
       " 'original': <tf.Tensor 'strided_slice_3:0' shape=(?, ?) dtype=int64>,\n",
       " 'transferred': <tf.Tensor 'attention_rnn_decoder_5/decoder/transpose_1:0' shape=(?, ?) dtype=int32>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 20):\n",
    "    # Train\n",
    "    iterator.restart_dataset(sess, ['train_g'])\n",
    "    _train_epoch(sess, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {iterator.handle: iterator.get_handle(sess, 'train_g')}\n",
    "result = sess.run(model.train_g_RL2,feed_dict=feed_dict)\n",
    "print('loss_g_RL2: ',result['loss_g_RL2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(model.samples,feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
