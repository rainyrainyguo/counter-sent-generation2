{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# pylint: disable=invalid-name, too-many-locals, too-many-arguments, no-member\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import texar as tx\n",
    "\n",
    "from ctrl_gen_model import CtrlGenModel\n",
    "\n",
    "\n",
    "config = importlib.import_module('config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guojy/texar/texar/modules/decoders/rnn_decoder_helpers.py:321: RelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:427: ExpRelaxedOneHotCategorical.__init__ (from tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/relaxed_onehot_categorical.py:429: Exp.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.exp) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bijectors/exp.py:73: PowerTransform.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.power_transform) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n"
     ]
    }
   ],
   "source": [
    "train_data = tx.data.MultiAlignedData(config.train_data)\n",
    "val_data = tx.data.MultiAlignedData(config.val_data)\n",
    "test_data = tx.data.MultiAlignedData(config.test_data)\n",
    "vocab = train_data.vocab(0)\n",
    "\n",
    "# Each training batch is used twice: once for updating the generator and\n",
    "# once for updating the discriminator. Feedable data iterator is used for\n",
    "# such case.\n",
    "iterator = tx.data.FeedableDataIterator(\n",
    "    {'train_g': train_data, 'train_d': train_data,\n",
    "     'val': val_data, 'test': test_data})\n",
    "batch = iterator.get_next()\n",
    "\n",
    "# Model\n",
    "gamma = tf.placeholder(dtype=tf.float32, shape=[], name='gamma')\n",
    "lambda_g = tf.placeholder(dtype=tf.float32, shape=[], name='lambda_g')\n",
    "model = CtrlGenModel(batch, vocab, gamma, lambda_g, config.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(sess, gamma_, lambda_g_, epoch, verbose=True):\n",
    "    avg_meters_d = tx.utils.AverageRecorder(size=10)\n",
    "    avg_meters_g = tx.utils.AverageRecorder(size=10)\n",
    "\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            step += 1\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'train_d'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_\n",
    "            }\n",
    "\n",
    "            vals_d = sess.run(model.fetches_train_d, feed_dict=feed_dict)\n",
    "            avg_meters_d.add(vals_d)\n",
    "\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, 'train_g'),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_\n",
    "            }\n",
    "            vals_g = sess.run(model.fetches_train_g, feed_dict=feed_dict)\n",
    "            avg_meters_g.add(vals_g)\n",
    "\n",
    "            if verbose and (step == 1 or step % 10 == 0):\n",
    "                print('step: {}, {}'.format(step, avg_meters_d.to_str(4)))\n",
    "                print('step: {}, {}'.format(step, avg_meters_g.to_str(4)))\n",
    "\n",
    "            if verbose and step % config.display_eval == 0:\n",
    "                iterator.restart_dataset(sess, 'val')\n",
    "                _eval_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('epoch: {}, {}'.format(epoch, avg_meters_d.to_str(4)))\n",
    "            print('epoch: {}, {}'.format(epoch, avg_meters_g.to_str(4)))\n",
    "            break\n",
    "\n",
    "def _eval_epoch(sess, gamma_, lambda_g_, epoch, val_or_test='val'):\n",
    "    avg_meters = tx.utils.AverageRecorder()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "            vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n",
    "\n",
    "            batch_size = vals.pop('batch_size')\n",
    "\n",
    "            # Computes BLEU\n",
    "            samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "            hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "            print(\"samples: \",hyps)\n",
    "\n",
    "            refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "            refs = np.expand_dims(refs, axis=1)\n",
    "            print(\"reference: \",refs)\n",
    "\n",
    "            bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "            vals['bleu'] = bleu\n",
    "\n",
    "            avg_meters.add(vals, weight=batch_size)\n",
    "\n",
    "            ###################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # Writes samples\n",
    "            tx.utils.write_paired_text(\n",
    "                refs.squeeze(), hyps,\n",
    "                os.path.join(config.sample_path, 'val.%d'%epoch),\n",
    "                append=True, mode='v')\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('{}: {}'.format(\n",
    "                val_or_test, avg_meters.to_str(precision=4)))\n",
    "            break\n",
    "\n",
    "    return avg_meters.avg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e09214fe0dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train_g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_g_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-d50121f9091f>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(sess, gamma_, lambda_g_, epoch, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mlambda_g\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlambda_g_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             }\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mvals_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches_train_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mavg_meters_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterator.restart_dataset(sess, ['train_g', 'train_d'])\n",
    "_train_epoch(sess, gamma_, lambda_g_, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile.MakeDirs(config.sample_path)\n",
    "tf.gfile.MakeDirs(config.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.initialize_dataset(sess)\n",
    "\n",
    "gamma_ = 1.\n",
    "lambda_g_ = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 1.0, lambda_g: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('gamma: {}, lambda_g: {}'.format(gamma_, lambda_g_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_d: 0.0832 accu_d: 0.9688\n",
      "step: 1, loss_g: 1.3574 loss_g_ae: 1.3574 loss_g_clas: 1.8153 accu_g: 0.4219 accu_g_gdy: 0.4531\n",
      "step: 10, loss_d: 0.1106 accu_d: 0.9625\n",
      "step: 10, loss_g: 1.2945 loss_g_ae: 1.2945 loss_g_clas: 2.1759 accu_g: 0.4016 accu_g_gdy: 0.3688\n",
      "step: 20, loss_d: 0.0738 accu_d: 0.9734\n",
      "step: 20, loss_g: 1.2504 loss_g_ae: 1.2504 loss_g_clas: 2.2360 accu_g: 0.4109 accu_g_gdy: 0.3922\n",
      "step: 30, loss_d: 0.0834 accu_d: 0.9719\n",
      "step: 30, loss_g: 1.2639 loss_g_ae: 1.2639 loss_g_clas: 2.4046 accu_g: 0.3812 accu_g_gdy: 0.3875\n",
      "step: 40, loss_d: 0.1122 accu_d: 0.9594\n",
      "step: 40, loss_g: 1.2858 loss_g_ae: 1.2858 loss_g_clas: 2.4674 accu_g: 0.3766 accu_g_gdy: 0.3812\n",
      "step: 50, loss_d: 0.0953 accu_d: 0.9656\n",
      "step: 50, loss_g: 1.1643 loss_g_ae: 1.1643 loss_g_clas: 2.3910 accu_g: 0.3812 accu_g_gdy: 0.3766\n",
      "step: 60, loss_d: 0.0967 accu_d: 0.9625\n",
      "step: 60, loss_g: 1.2551 loss_g_ae: 1.2551 loss_g_clas: 2.1699 accu_g: 0.4016 accu_g_gdy: 0.4078\n",
      "step: 70, loss_d: 0.0729 accu_d: 0.9766\n",
      "step: 70, loss_g: 1.1886 loss_g_ae: 1.1886 loss_g_clas: 2.4866 accu_g: 0.3750 accu_g_gdy: 0.3594\n",
      "step: 80, loss_d: 0.0782 accu_d: 0.9656\n",
      "step: 80, loss_g: 1.1690 loss_g_ae: 1.1690 loss_g_clas: 2.4263 accu_g: 0.3563 accu_g_gdy: 0.3859\n",
      "step: 90, loss_d: 0.0800 accu_d: 0.9766\n",
      "step: 90, loss_g: 1.2109 loss_g_ae: 1.2109 loss_g_clas: 2.5936 accu_g: 0.3719 accu_g_gdy: 0.3641\n",
      "step: 100, loss_d: 0.0751 accu_d: 0.9656\n",
      "step: 100, loss_g: 1.1046 loss_g_ae: 1.1046 loss_g_clas: 2.4055 accu_g: 0.3812 accu_g_gdy: 0.3656\n",
      "step: 110, loss_d: 0.0809 accu_d: 0.9641\n",
      "step: 110, loss_g: 1.1124 loss_g_ae: 1.1124 loss_g_clas: 2.4062 accu_g: 0.3969 accu_g_gdy: 0.3406\n",
      "step: 120, loss_d: 0.0884 accu_d: 0.9703\n",
      "step: 120, loss_g: 1.0328 loss_g_ae: 1.0328 loss_g_clas: 2.6644 accu_g: 0.3812 accu_g_gdy: 0.3422\n",
      "step: 130, loss_d: 0.0883 accu_d: 0.9703\n",
      "step: 130, loss_g: 1.1053 loss_g_ae: 1.1053 loss_g_clas: 2.8235 accu_g: 0.3484 accu_g_gdy: 0.3563\n",
      "step: 140, loss_d: 0.1006 accu_d: 0.9594\n",
      "step: 140, loss_g: 1.1011 loss_g_ae: 1.1011 loss_g_clas: 3.0600 accu_g: 0.3031 accu_g_gdy: 0.3344\n",
      "step: 150, loss_d: 0.1148 accu_d: 0.9609\n",
      "step: 150, loss_g: 1.0150 loss_g_ae: 1.0150 loss_g_clas: 2.8004 accu_g: 0.3422 accu_g_gdy: 0.3578\n",
      "step: 160, loss_d: 0.0791 accu_d: 0.9656\n",
      "step: 160, loss_g: 1.0903 loss_g_ae: 1.0903 loss_g_clas: 2.8600 accu_g: 0.3375 accu_g_gdy: 0.3187\n",
      "step: 170, loss_d: 0.1076 accu_d: 0.9625\n",
      "step: 170, loss_g: 1.0580 loss_g_ae: 1.0580 loss_g_clas: 3.0735 accu_g: 0.2906 accu_g_gdy: 0.3047\n",
      "step: 180, loss_d: 0.0962 accu_d: 0.9594\n",
      "step: 180, loss_g: 1.1000 loss_g_ae: 1.1000 loss_g_clas: 2.8013 accu_g: 0.3391 accu_g_gdy: 0.3594\n",
      "step: 190, loss_d: 0.0898 accu_d: 0.9672\n",
      "step: 190, loss_g: 1.0750 loss_g_ae: 1.0750 loss_g_clas: 2.7635 accu_g: 0.3047 accu_g_gdy: 0.3344\n",
      "step: 200, loss_d: 0.1006 accu_d: 0.9656\n",
      "step: 200, loss_g: 0.9807 loss_g_ae: 0.9807 loss_g_clas: 2.8043 accu_g: 0.3219 accu_g_gdy: 0.2891\n",
      "step: 210, loss_d: 0.0982 accu_d: 0.9641\n",
      "step: 210, loss_g: 0.9774 loss_g_ae: 0.9774 loss_g_clas: 2.6083 accu_g: 0.3266 accu_g_gdy: 0.2875\n",
      "step: 220, loss_d: 0.0932 accu_d: 0.9672\n",
      "step: 220, loss_g: 1.0410 loss_g_ae: 1.0410 loss_g_clas: 2.5311 accu_g: 0.3516 accu_g_gdy: 0.3500\n",
      "step: 230, loss_d: 0.1069 accu_d: 0.9703\n",
      "step: 230, loss_g: 1.0266 loss_g_ae: 1.0266 loss_g_clas: 2.8920 accu_g: 0.3297 accu_g_gdy: 0.3312\n",
      "step: 240, loss_d: 0.0788 accu_d: 0.9688\n",
      "step: 240, loss_g: 0.9557 loss_g_ae: 0.9557 loss_g_clas: 2.9091 accu_g: 0.3047 accu_g_gdy: 0.3078\n",
      "step: 250, loss_d: 0.0850 accu_d: 0.9750\n",
      "step: 250, loss_g: 1.0284 loss_g_ae: 1.0284 loss_g_clas: 2.9328 accu_g: 0.3063 accu_g_gdy: 0.3016\n",
      "step: 260, loss_d: 0.1091 accu_d: 0.9563\n",
      "step: 260, loss_g: 1.0204 loss_g_ae: 1.0204 loss_g_clas: 2.8003 accu_g: 0.3312 accu_g_gdy: 0.3328\n",
      "step: 270, loss_d: 0.0883 accu_d: 0.9656\n",
      "step: 270, loss_g: 1.0674 loss_g_ae: 1.0674 loss_g_clas: 2.9037 accu_g: 0.2766 accu_g_gdy: 0.3078\n",
      "step: 280, loss_d: 0.0979 accu_d: 0.9625\n",
      "step: 280, loss_g: 1.0577 loss_g_ae: 1.0577 loss_g_clas: 2.9416 accu_g: 0.3109 accu_g_gdy: 0.3469\n",
      "step: 290, loss_d: 0.0903 accu_d: 0.9656\n",
      "step: 290, loss_g: 1.7125 loss_g_ae: 1.7125 loss_g_clas: 2.5468 accu_g: 0.3281 accu_g_gdy: 0.3047\n",
      "step: 300, loss_d: 0.1012 accu_d: 0.9578\n",
      "step: 300, loss_g: 1.2778 loss_g_ae: 1.2778 loss_g_clas: 2.6174 accu_g: 0.3344 accu_g_gdy: 0.3281\n",
      "step: 310, loss_d: 0.0946 accu_d: 0.9672\n",
      "step: 310, loss_g: 1.1783 loss_g_ae: 1.1783 loss_g_clas: 2.8285 accu_g: 0.2938 accu_g_gdy: 0.2922\n",
      "step: 320, loss_d: 0.0768 accu_d: 0.9688\n",
      "step: 320, loss_g: 1.0410 loss_g_ae: 1.0410 loss_g_clas: 2.7640 accu_g: 0.3297 accu_g_gdy: 0.3563\n",
      "step: 330, loss_d: 0.0688 accu_d: 0.9703\n",
      "step: 330, loss_g: 1.0233 loss_g_ae: 1.0233 loss_g_clas: 3.0993 accu_g: 0.2703 accu_g_gdy: 0.3000\n",
      "step: 340, loss_d: 0.0757 accu_d: 0.9719\n",
      "step: 340, loss_g: 0.9648 loss_g_ae: 0.9648 loss_g_clas: 3.0249 accu_g: 0.3047 accu_g_gdy: 0.2750\n",
      "step: 350, loss_d: 0.0923 accu_d: 0.9641\n",
      "step: 350, loss_g: 0.9567 loss_g_ae: 0.9567 loss_g_clas: 3.1156 accu_g: 0.2875 accu_g_gdy: 0.2953\n",
      "step: 360, loss_d: 0.1120 accu_d: 0.9563\n",
      "step: 360, loss_g: 0.9088 loss_g_ae: 0.9088 loss_g_clas: 3.0960 accu_g: 0.2844 accu_g_gdy: 0.2922\n",
      "step: 370, loss_d: 0.0758 accu_d: 0.9703\n",
      "step: 370, loss_g: 0.8830 loss_g_ae: 0.8830 loss_g_clas: 3.0856 accu_g: 0.2969 accu_g_gdy: 0.3125\n",
      "step: 380, loss_d: 0.1069 accu_d: 0.9609\n",
      "step: 380, loss_g: 0.9609 loss_g_ae: 0.9609 loss_g_clas: 2.9539 accu_g: 0.2969 accu_g_gdy: 0.3547\n",
      "step: 390, loss_d: 0.0735 accu_d: 0.9703\n",
      "step: 390, loss_g: 0.9042 loss_g_ae: 0.9042 loss_g_clas: 3.0521 accu_g: 0.2859 accu_g_gdy: 0.2938\n",
      "step: 400, loss_d: 0.1090 accu_d: 0.9609\n",
      "step: 400, loss_g: 0.9074 loss_g_ae: 0.9074 loss_g_clas: 3.0367 accu_g: 0.2891 accu_g_gdy: 0.2891\n",
      "step: 410, loss_d: 0.1069 accu_d: 0.9703\n",
      "step: 410, loss_g: 0.8945 loss_g_ae: 0.8945 loss_g_clas: 2.9890 accu_g: 0.2812 accu_g_gdy: 0.3078\n",
      "step: 420, loss_d: 0.0795 accu_d: 0.9656\n",
      "step: 420, loss_g: 0.8694 loss_g_ae: 0.8694 loss_g_clas: 3.0885 accu_g: 0.2703 accu_g_gdy: 0.2781\n",
      "step: 430, loss_d: 0.0951 accu_d: 0.9672\n",
      "step: 430, loss_g: 0.8504 loss_g_ae: 0.8504 loss_g_clas: 3.1811 accu_g: 0.3000 accu_g_gdy: 0.2828\n",
      "step: 440, loss_d: 0.0732 accu_d: 0.9750\n",
      "step: 440, loss_g: 0.8665 loss_g_ae: 0.8665 loss_g_clas: 3.1102 accu_g: 0.2938 accu_g_gdy: 0.2969\n",
      "step: 450, loss_d: 0.0967 accu_d: 0.9656\n",
      "step: 450, loss_g: 0.8369 loss_g_ae: 0.8369 loss_g_clas: 3.0663 accu_g: 0.2984 accu_g_gdy: 0.2891\n",
      "step: 460, loss_d: 0.0764 accu_d: 0.9641\n",
      "step: 460, loss_g: 0.8101 loss_g_ae: 0.8101 loss_g_clas: 3.1796 accu_g: 0.2609 accu_g_gdy: 0.2687\n",
      "step: 470, loss_d: 0.0794 accu_d: 0.9750\n",
      "step: 470, loss_g: 0.8027 loss_g_ae: 0.8027 loss_g_clas: 2.9656 accu_g: 0.3109 accu_g_gdy: 0.2953\n",
      "step: 480, loss_d: 0.0724 accu_d: 0.9703\n",
      "step: 480, loss_g: 0.7809 loss_g_ae: 0.7809 loss_g_clas: 3.1578 accu_g: 0.2844 accu_g_gdy: 0.2875\n",
      "step: 490, loss_d: 0.0823 accu_d: 0.9641\n",
      "step: 490, loss_g: 0.8452 loss_g_ae: 0.8452 loss_g_clas: 3.1380 accu_g: 0.2906 accu_g_gdy: 0.2828\n",
      "step: 500, loss_d: 0.0727 accu_d: 0.9797\n",
      "step: 500, loss_g: 0.7922 loss_g_ae: 0.7922 loss_g_clas: 3.3545 accu_g: 0.2953 accu_g_gdy: 0.2703\n",
      "step: 510, loss_d: 0.0479 accu_d: 0.9875\n",
      "step: 510, loss_g: 0.8576 loss_g_ae: 0.8576 loss_g_clas: 3.4539 accu_g: 0.2703 accu_g_gdy: 0.2906\n",
      "step: 520, loss_d: 0.0788 accu_d: 0.9703\n",
      "step: 520, loss_g: 0.8080 loss_g_ae: 0.8080 loss_g_clas: 3.4391 accu_g: 0.2844 accu_g_gdy: 0.2703\n",
      "step: 530, loss_d: 0.0631 accu_d: 0.9781\n",
      "step: 530, loss_g: 0.7884 loss_g_ae: 0.7884 loss_g_clas: 3.4681 accu_g: 0.2766 accu_g_gdy: 0.3016\n",
      "step: 540, loss_d: 0.0859 accu_d: 0.9750\n",
      "step: 540, loss_g: 0.7735 loss_g_ae: 0.7735 loss_g_clas: 3.3054 accu_g: 0.2687 accu_g_gdy: 0.2844\n",
      "step: 550, loss_d: 0.0954 accu_d: 0.9594\n",
      "step: 550, loss_g: 0.7969 loss_g_ae: 0.7969 loss_g_clas: 3.2578 accu_g: 0.2750 accu_g_gdy: 0.2703\n",
      "step: 560, loss_d: 0.0654 accu_d: 0.9766\n",
      "step: 560, loss_g: 0.8708 loss_g_ae: 0.8708 loss_g_clas: 3.4170 accu_g: 0.2656 accu_g_gdy: 0.2875\n",
      "step: 570, loss_d: 0.0630 accu_d: 0.9766\n",
      "step: 570, loss_g: 0.7463 loss_g_ae: 0.7463 loss_g_clas: 3.4822 accu_g: 0.2578 accu_g_gdy: 0.2516\n",
      "step: 580, loss_d: 0.0706 accu_d: 0.9750\n",
      "step: 580, loss_g: 0.8330 loss_g_ae: 0.8330 loss_g_clas: 3.3910 accu_g: 0.3094 accu_g_gdy: 0.3000\n",
      "step: 590, loss_d: 0.0968 accu_d: 0.9719\n",
      "step: 590, loss_g: 0.8027 loss_g_ae: 0.8027 loss_g_clas: 3.3775 accu_g: 0.2594 accu_g_gdy: 0.2719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 600, loss_d: 0.0544 accu_d: 0.9750\n",
      "step: 600, loss_g: 0.7937 loss_g_ae: 0.7937 loss_g_clas: 3.4107 accu_g: 0.2922 accu_g_gdy: 0.2891\n",
      "step: 610, loss_d: 0.0683 accu_d: 0.9844\n",
      "step: 610, loss_g: 0.8060 loss_g_ae: 0.8060 loss_g_clas: 3.4462 accu_g: 0.2328 accu_g_gdy: 0.2641\n",
      "step: 620, loss_d: 0.0941 accu_d: 0.9672\n",
      "step: 620, loss_g: 0.8092 loss_g_ae: 0.8092 loss_g_clas: 3.4596 accu_g: 0.2594 accu_g_gdy: 0.2531\n",
      "step: 630, loss_d: 0.0773 accu_d: 0.9703\n",
      "step: 630, loss_g: 0.8320 loss_g_ae: 0.8320 loss_g_clas: 3.6549 accu_g: 0.2500 accu_g_gdy: 0.2609\n",
      "step: 640, loss_d: 0.1109 accu_d: 0.9641\n",
      "step: 640, loss_g: 0.7936 loss_g_ae: 0.7936 loss_g_clas: 3.7584 accu_g: 0.2203 accu_g_gdy: 0.2344\n",
      "step: 650, loss_d: 0.0918 accu_d: 0.9578\n",
      "step: 650, loss_g: 0.7552 loss_g_ae: 0.7552 loss_g_clas: 3.7028 accu_g: 0.2266 accu_g_gdy: 0.2281\n",
      "step: 660, loss_d: 0.0782 accu_d: 0.9734\n",
      "step: 660, loss_g: 0.8139 loss_g_ae: 0.8139 loss_g_clas: 3.5512 accu_g: 0.2609 accu_g_gdy: 0.2422\n",
      "step: 670, loss_d: 0.1000 accu_d: 0.9656\n",
      "step: 670, loss_g: 0.8024 loss_g_ae: 0.8024 loss_g_clas: 3.7458 accu_g: 0.2500 accu_g_gdy: 0.2562\n",
      "step: 680, loss_d: 0.1185 accu_d: 0.9625\n",
      "step: 680, loss_g: 0.8486 loss_g_ae: 0.8486 loss_g_clas: 3.5681 accu_g: 0.2734 accu_g_gdy: 0.2719\n",
      "step: 690, loss_d: 0.0832 accu_d: 0.9672\n",
      "step: 690, loss_g: 0.8233 loss_g_ae: 0.8233 loss_g_clas: 3.5846 accu_g: 0.2500 accu_g_gdy: 0.2672\n",
      "step: 700, loss_d: 0.0929 accu_d: 0.9563\n",
      "step: 700, loss_g: 0.8473 loss_g_ae: 0.8473 loss_g_clas: 3.4550 accu_g: 0.2234 accu_g_gdy: 0.2437\n",
      "step: 710, loss_d: 0.0729 accu_d: 0.9688\n",
      "step: 710, loss_g: 0.7894 loss_g_ae: 0.7894 loss_g_clas: 3.5970 accu_g: 0.2234 accu_g_gdy: 0.2453\n",
      "step: 720, loss_d: 0.1084 accu_d: 0.9625\n",
      "step: 720, loss_g: 0.7692 loss_g_ae: 0.7692 loss_g_clas: 3.7316 accu_g: 0.2375 accu_g_gdy: 0.2391\n",
      "step: 730, loss_d: 0.0737 accu_d: 0.9750\n",
      "step: 730, loss_g: 0.7002 loss_g_ae: 0.7002 loss_g_clas: 3.5975 accu_g: 0.2281 accu_g_gdy: 0.2375\n",
      "step: 740, loss_d: 0.0893 accu_d: 0.9656\n",
      "step: 740, loss_g: 0.7164 loss_g_ae: 0.7164 loss_g_clas: 3.6986 accu_g: 0.2359 accu_g_gdy: 0.2313\n",
      "step: 750, loss_d: 0.0962 accu_d: 0.9688\n",
      "step: 750, loss_g: 0.7415 loss_g_ae: 0.7415 loss_g_clas: 3.6217 accu_g: 0.2672 accu_g_gdy: 0.2422\n",
      "step: 760, loss_d: 0.0872 accu_d: 0.9656\n",
      "step: 760, loss_g: 0.7171 loss_g_ae: 0.7171 loss_g_clas: 3.5201 accu_g: 0.2500 accu_g_gdy: 0.2359\n",
      "step: 770, loss_d: 0.0926 accu_d: 0.9672\n",
      "step: 770, loss_g: 0.7551 loss_g_ae: 0.7551 loss_g_clas: 3.4265 accu_g: 0.2500 accu_g_gdy: 0.2391\n",
      "step: 780, loss_d: 0.0756 accu_d: 0.9656\n",
      "step: 780, loss_g: 0.7305 loss_g_ae: 0.7305 loss_g_clas: 3.5454 accu_g: 0.2594 accu_g_gdy: 0.2922\n",
      "step: 790, loss_d: 0.1158 accu_d: 0.9578\n",
      "step: 790, loss_g: 0.7277 loss_g_ae: 0.7277 loss_g_clas: 3.4592 accu_g: 0.2453 accu_g_gdy: 0.2437\n",
      "step: 800, loss_d: 0.1047 accu_d: 0.9625\n",
      "step: 800, loss_g: 0.7743 loss_g_ae: 0.7743 loss_g_clas: 3.4942 accu_g: 0.2422 accu_g_gdy: 0.2375\n",
      "step: 810, loss_d: 0.0857 accu_d: 0.9672\n",
      "step: 810, loss_g: 0.7535 loss_g_ae: 0.7535 loss_g_clas: 3.6180 accu_g: 0.2203 accu_g_gdy: 0.2328\n",
      "step: 820, loss_d: 0.1004 accu_d: 0.9625\n",
      "step: 820, loss_g: 0.7902 loss_g_ae: 0.7902 loss_g_clas: 3.3934 accu_g: 0.2547 accu_g_gdy: 0.2281\n",
      "step: 830, loss_d: 0.1121 accu_d: 0.9500\n",
      "step: 830, loss_g: 0.7179 loss_g_ae: 0.7179 loss_g_clas: 3.6759 accu_g: 0.2016 accu_g_gdy: 0.2297\n",
      "step: 840, loss_d: 0.0948 accu_d: 0.9656\n",
      "step: 840, loss_g: 0.6945 loss_g_ae: 0.6945 loss_g_clas: 3.5978 accu_g: 0.2422 accu_g_gdy: 0.2578\n",
      "step: 850, loss_d: 0.0913 accu_d: 0.9609\n",
      "step: 850, loss_g: 0.6757 loss_g_ae: 0.6757 loss_g_clas: 3.7422 accu_g: 0.2250 accu_g_gdy: 0.2172\n",
      "step: 860, loss_d: 0.0727 accu_d: 0.9688\n",
      "step: 860, loss_g: 0.7229 loss_g_ae: 0.7229 loss_g_clas: 3.4030 accu_g: 0.2359 accu_g_gdy: 0.2391\n",
      "step: 870, loss_d: 0.0852 accu_d: 0.9672\n",
      "step: 870, loss_g: 0.7164 loss_g_ae: 0.7164 loss_g_clas: 3.4841 accu_g: 0.2531 accu_g_gdy: 0.2297\n",
      "step: 880, loss_d: 0.0788 accu_d: 0.9688\n",
      "step: 880, loss_g: 0.7178 loss_g_ae: 0.7178 loss_g_clas: 3.5372 accu_g: 0.2641 accu_g_gdy: 0.2500\n",
      "step: 890, loss_d: 0.0898 accu_d: 0.9688\n",
      "step: 890, loss_g: 0.7032 loss_g_ae: 0.7032 loss_g_clas: 3.3955 accu_g: 0.2687 accu_g_gdy: 0.2437\n",
      "step: 900, loss_d: 0.0786 accu_d: 0.9719\n",
      "step: 900, loss_g: 0.7255 loss_g_ae: 0.7255 loss_g_clas: 3.5983 accu_g: 0.2375 accu_g_gdy: 0.2437\n",
      "step: 910, loss_d: 0.1194 accu_d: 0.9641\n",
      "step: 910, loss_g: 0.7865 loss_g_ae: 0.7865 loss_g_clas: 3.6915 accu_g: 0.2062 accu_g_gdy: 0.2328\n",
      "step: 920, loss_d: 0.0955 accu_d: 0.9656\n",
      "step: 920, loss_g: 0.7023 loss_g_ae: 0.7023 loss_g_clas: 3.3881 accu_g: 0.2562 accu_g_gdy: 0.2391\n",
      "step: 930, loss_d: 0.0914 accu_d: 0.9688\n",
      "step: 930, loss_g: 0.7188 loss_g_ae: 0.7188 loss_g_clas: 3.2895 accu_g: 0.2672 accu_g_gdy: 0.2672\n",
      "step: 940, loss_d: 0.0861 accu_d: 0.9594\n",
      "step: 940, loss_g: 0.6816 loss_g_ae: 0.6816 loss_g_clas: 3.6112 accu_g: 0.2266 accu_g_gdy: 0.2203\n",
      "step: 950, loss_d: 0.1002 accu_d: 0.9609\n",
      "step: 950, loss_g: 0.6511 loss_g_ae: 0.6511 loss_g_clas: 3.4388 accu_g: 0.2703 accu_g_gdy: 0.2516\n",
      "step: 960, loss_d: 0.0668 accu_d: 0.9734\n",
      "step: 960, loss_g: 0.6855 loss_g_ae: 0.6855 loss_g_clas: 3.5432 accu_g: 0.2172 accu_g_gdy: 0.2172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e9f8e89ef40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train_g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_g_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-d50121f9091f>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(sess, gamma_, lambda_g_, epoch, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mlambda_g\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlambda_g_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             }\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mvals_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches_train_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mavg_meters_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,10):\n",
    "    iterator.restart_dataset(sess, ['train_g', 'train_d'])\n",
    "    _train_epoch(sess, gamma_, lambda_g_, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.restart_dataset(sess, 'val')\n",
    "#_eval_epoch(sess, gamma_, lambda_g_, epoch, 'val')\n",
    "val_or_test='val'\n",
    "avg_meters = tx.utils.AverageRecorder()\n",
    "feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "vals = sess.run(model.fetches_eval, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'loss_g': 0.5256332,\n",
       " 'loss_g_ae': 0.5256332,\n",
       " 'loss_g_clas': 4.253072,\n",
       " 'loss_d': 0.052145958,\n",
       " 'accu_d': 0.96875,\n",
       " 'accu_g': 0.171875,\n",
       " 'accu_g_gdy': 0.078125,\n",
       " 'original': array([[ 300,   31,  173, ...,   17,    8,    2],\n",
       "        [  25,  353,   70, ...,    0,    0,    0],\n",
       "        [   7,   11,  201, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1097,    2,    0, ...,    0,    0,    0],\n",
       "        [  86,  226,   38, ...,    2,    0,    0],\n",
       "        [   7,  119,  582, ...,    2,    0,    0]]),\n",
       " 'transferred': array([[ 300,   31,  173, ...,   17,    8,    2],\n",
       "        [  25,  353,   70, ...,    0,    0,    0],\n",
       "        [   7,   11,  201, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [1097,    2,    0, ...,    0,    0,    0],\n",
       "        [  86,  226,   38, ...,    2,    0,    0],\n",
       "        [   7,  119, 1017, ...,    2,    0,    0]], dtype=int32)}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'loss_g': 1.1538124,\n",
       " 'loss_g_ae': 1.1538124,\n",
       " 'loss_g_clas': 2.4324093,\n",
       " 'loss_d': 0.09194705,\n",
       " 'accu_d': 0.953125,\n",
       " 'accu_g': 0.40625,\n",
       " 'accu_g_gdy': 0.359375,\n",
       " 'original': array([[ 365,  139,    6, ...,    0,    0,    0],\n",
       "        [3339, 2903,    5, ...,    2,    0,    0],\n",
       "        [   5,   17,    6, ...,    2,    0,    0],\n",
       "        ...,\n",
       "        [  67,  388,   30, ...,    0,    0,    0],\n",
       "        [  15,  333,   10, ...,    0,    0,    0],\n",
       "        [  35,  101,   63, ...,    0,    0,    0]]),\n",
       " 'transferred': array([[185, 139,   6, ...,   0,   0,   0],\n",
       "        [658, 253,   5, ...,   0,   0,   0],\n",
       "        [  5,  17,   6, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 67, 306,  30, ...,   0,   0,   0],\n",
       "        [ 15,  19,  10, ...,   0,   0,   0],\n",
       "        [ 35, 101,  63, ...,   0,   0,   0]], dtype=int32)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'loss_g': 4.487425,\n",
       " 'loss_g_ae': 4.487425,\n",
       " 'loss_g_clas': 0.2423923,\n",
       " 'loss_d': 0.1387409,\n",
       " 'accu_d': 0.953125,\n",
       " 'accu_g': 0.953125,\n",
       " 'accu_g_gdy': 0.953125,\n",
       " 'original': array([[   7,   65,   31, ...,    0,    0,    0],\n",
       "        [1284,   55,   22, ...,    0,    0,    0],\n",
       "        [3684,   10,  258, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   7,   81,    5, ...,    0,    0,    0],\n",
       "        [   7,  119,   52, ...,    0,    0,    0],\n",
       "        [  21,  276,    8, ...,    0,    0,    0]]),\n",
       " 'transferred': array([[ 5,  5,  5, ...,  0,  0,  0],\n",
       "        [ 5,  5, 11, ...,  0,  0,  0],\n",
       "        [ 5,  5, 11, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 5,  5,  5, ...,  0,  0,  0],\n",
       "        [ 5, 10, 10, ...,  0,  0,  0],\n",
       "        [20,  4,  4, ...,  0,  0,  0]], dtype=int32)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'loss_g': 4.594885,\n",
       " 'loss_g_ae': 4.594885,\n",
       " 'loss_g_clas': 0.36999792,\n",
       " 'loss_d': 0.1608523,\n",
       " 'accu_d': 0.9375,\n",
       " 'accu_g': 0.921875,\n",
       " 'accu_g_gdy': 0.640625,\n",
       " 'original': array([[  20,   10,   45, ...,    0,    0,    0],\n",
       "        [  12,   25,  871, ...,    0,    0,    0],\n",
       "        [  23, 3893,  436, ...,  896,    4,    2],\n",
       "        ...,\n",
       "        [  56, 1387,  112, ...,    0,    0,    0],\n",
       "        [1190,    6, 2502, ...,    0,    0,    0],\n",
       "        [  15,   10,   12, ...,    0,    0,    0]]),\n",
       " 'transferred': array([[ 7,  5,  5, ...,  0,  0,  0],\n",
       "        [ 7,  5,  5, ...,  0,  0,  0],\n",
       "        [ 5,  5,  5, ...,  4,  4,  2],\n",
       "        ...,\n",
       "        [ 5, 16, 16, ...,  0,  0,  0],\n",
       "        [ 5,  5,  6, ...,  0,  0,  0],\n",
       "        [ 7,  5,  5, ...,  0,  0,  0]], dtype=int32)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'loss_g': 4.973015,\n",
       " 'loss_g_ae': 4.973015,\n",
       " 'loss_g_clas': 0.70567334,\n",
       " 'loss_d': 0.16262153,\n",
       " 'accu_d': 0.953125,\n",
       " 'accu_g': 0.609375,\n",
       " 'accu_g_gdy': 0.6875,\n",
       " 'original': array([[  7, 177,  37, ...,   0,   0,   0],\n",
       "        [240,  27, 670, ...,   0,   0,   0],\n",
       "        [  7,  65,  28, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [250, 125,   4, ...,   0,   0,   0],\n",
       "        [258,  12,  55, ...,   0,   0,   0],\n",
       "        [ 25, 184,   9, ...,   0,   0,   0]]),\n",
       " 'transferred': array([[ 7,  5,  5, ...,  0,  0,  0],\n",
       "        [ 7,  5,  5, ...,  0,  0,  0],\n",
       "        [ 5,  5,  5, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [16,  8,  8, ...,  0,  0,  0],\n",
       "        [ 5,  5,  5, ...,  0,  0,  0],\n",
       "        [ 5,  5,  5, ...,  0,  0,  0]], dtype=int32)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = vals.pop('batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  ['off work and they are ever recommended !'\n",
      " 'needless took the front desk is a tasted doctors ... ever too .'\n",
      " 'the food and service at this location has always been on selection .'\n",
      " 'service selection on providers and in line , and a flavorful staff .'\n",
      " 'a sure of asking checked restaurants in rice town specials .'\n",
      " 'this place is awesome .' 'the menu is pretty highlight .'\n",
      " 'plenty of chips places around with better works , and nice food .'\n",
      " 'they are sour helpful , always friendly , always interested .'\n",
      " 'the person cream and service are over good too .'\n",
      " \"thank 's booths with the burgh .\" 'i have always why this sign bell .'\n",
      " 'my : its way too consistently in perfect .'\n",
      " 'great people there and they all taste very friendly .'\n",
      " 'pretty good food !' 'no bueno !'\n",
      " 'to get any room of service , you need to any at the bar area .'\n",
      " 'the food was great , service friendly and spot on .'\n",
      " \"love my dropped 's your doctors .\" 'so so disappointed .'\n",
      " 'i wish continue eat was an chain .' 'i do not recommend this practice .'\n",
      " 'will have to enjoy it .' 'not impressed .'\n",
      " 'hands night , we were the only _num_ in the restaurant .'\n",
      " \"they tell me it 's going to be support me for wrong .\"\n",
      " 'this everyone was pretty quite and rude .'\n",
      " \"i 'll when i am because asking , so i upgrade .\"\n",
      " 'one of the lobster practice around the valley .'\n",
      " \"they , you 'll get tires years , quick items and doing your etc !\"\n",
      " 'food was very good with equally tasting specials .'\n",
      " 'gross and sour are great .'\n",
      " 'would highly recommend for different night or asking hours for a drink !'\n",
      " 'staff is super friendly and so helpful .'\n",
      " 'i came in items and the staff was so friendly and helpful .'\n",
      " 'i was probably enough by the customer service i am .'\n",
      " 'the customer service is doctors !'\n",
      " 'great food , the pizza yet , and over salads .'\n",
      " 'there was a bad but it was worth it .'\n",
      " 'the manager , though it looked getting plain , never had very good work .'\n",
      " 'a some hour food met is drive .'\n",
      " 'my favorite made _num_ dropped forget out than _num_ years .'\n",
      " 'in top , he value .' 'it park , more like it !'\n",
      " 'they have a great selection of table , over lunch and clean .'\n",
      " 'it has the most soup atmosphere .'\n",
      " 'this sushi is a reasonably for doctors .'\n",
      " 'took favorite of sauce and about _num_ minutes later , we are me .'\n",
      " 'every time i come here the service is really bad .'\n",
      " \"i 'll by zero be back on my he reviews to vegas !\"\n",
      " 'should down no night but still great !' 'great is place .'\n",
      " 'on on of that she know asked to to what the poor was doing .'\n",
      " 'service service , friendly staff , professional .'\n",
      " 'highly recommended !' 'just horrible food !'\n",
      " 'nice , place but rude staff .' 'really poor and chain .'\n",
      " 'finally a decent place to order tires from !' 'the wings park .'\n",
      " 'great slow .'\n",
      " 'one office with available chairs vodka moving park and cheese .'\n",
      " 'this place is a outstanding place to do business with .'\n",
      " 'we highly recommend this gross .']\n"
     ]
    }
   ],
   "source": [
    "# Computes BLEU\n",
    "samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "print(\"samples: \",hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference:  [['honest work and they are highly recommended !']\n",
      " ['josh @ the front desk is a real hoot ... fun guy .']\n",
      " ['the food and service at this location has always been top notch .']\n",
      " ['excellent selection on tap and in bottles , and a knowledgeable staff .']\n",
      " ['a bit of touristy southwest heaven in old town scottsdale .']\n",
      " ['this place is awesome .']\n",
      " ['the menu is pretty extensive .']\n",
      " ['tons of sandwich places around with better attitudes , and better food .']\n",
      " ['they are soooo helpful , always friendly , always cheerful .']\n",
      " ['the ice cream and service are both good too .']\n",
      " [\"let 's begin with the positives .\"]\n",
      " ['i have always used this discount tire .']\n",
      " ['2nd : its way too dark in night .']\n",
      " ['great people there and they all seemed very friendly .']\n",
      " ['pretty good food !']\n",
      " ['no dice !']\n",
      " ['to get any kind of service , you need to sit at the bar .']\n",
      " ['the food was great , service friendly and spot on .']\n",
      " [\"love my mother 's day pie .\"]\n",
      " ['so so disappointed .']\n",
      " ['i wish zero stars was an option .']\n",
      " ['i do not recommend this cleaners .']\n",
      " ['will try to avoid it .']\n",
      " ['not impressed .']\n",
      " ['monday night , we were the only _num_ in the restaurant .']\n",
      " [\"they tell me it 's going to be 30-40 minutes for delivery .\"]\n",
      " ['however everyone was pretty short and rude .']\n",
      " ['i know when i am being scammed , so i declined .']\n",
      " ['one of the premier hotels around the valley .']\n",
      " [\"plus , you 'll get expert help , quick repairs and support your community !\"]\n",
      " ['food was very good with ample serving size .']\n",
      " ['gym and pool are great .']\n",
      " ['would highly recommend for date night or meeting friends for a drink !']\n",
      " ['staff is super friendly and so helpful .']\n",
      " ['i came in early and the staff was so friendly and helpful .']\n",
      " ['i was pleasantly surprised by the customer service i received .']\n",
      " ['the customer service is nonexistent !']\n",
      " ['great food , the pizza especially , and huge salads .']\n",
      " ['there was a wait but it was worth it .']\n",
      " ['the burrito , though it looked fairly plain , actually had very good flavor .']\n",
      " ['a half hour food ticket is sad .']\n",
      " ['my daughters $ _num_ crown fell out within _num_ weeks .']\n",
      " ['in short , he sucked .']\n",
      " ['big surf , more like big disappointment .']\n",
      " ['they have a great selection of entrees , both lunch and breakfast .']\n",
      " ['it has the most unique atmosphere .']\n",
      " ['this theater is a disaster for acoustics .']\n",
      " ['two rounds of drinks and about _num_ minutes later , we are seated .']\n",
      " ['every time i come here the service is really bad .']\n",
      " [\"i 'll most def be back on my next trip to phx !\"]\n",
      " ['packed house last night but still great !']\n",
      " ['great local place .']\n",
      " ['on top of that she actually stopped to hear what the customer was saying .']\n",
      " ['excellent service , friendly staff , professional .']\n",
      " ['highly recommended !']\n",
      " ['just excellent food !']\n",
      " ['nice little place but rude staff .']\n",
      " ['really small and cozy .']\n",
      " ['finally a decent place to order take-out from !']\n",
      " ['the wings rule .']\n",
      " ['great burger .']\n",
      " ['one burger with onions pickles mayo mustard ketchup and cheese .']\n",
      " ['this company is a outstanding company to do business with .']\n",
      " ['we highly recommend this gym .']]\n"
     ]
    }
   ],
   "source": [
    "refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "refs = np.expand_dims(refs, axis=1)\n",
    "print(\"reference: \",refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  [\"ca n't wait to come back to watch some anyone and eat amazing food !\"\n",
      " 'very pleasant experience .'\n",
      " 'i was extremely pleased with my experience .'\n",
      " 'although the melting are a little expensive , it is nor worth it .'\n",
      " 'will definitely be back !' 'no , no , no .' 'not fancy .' 'wow !'\n",
      " 'i am so not overwhelming !'\n",
      " 'its sweet with that brown answering practices .'\n",
      " \"i 've eaten a lot of it in my day .\" 'our bartender was very nice .'\n",
      " 'so looking not worth it .'\n",
      " 'no server with the food here , just the service .'\n",
      " 'service was really slow .'\n",
      " 'if i could give it a negative star i would in a safety !'\n",
      " 'i have continue the owner several times with no return phone call .'\n",
      " 'i thought and moist and found a gem for the kids to enjoy .'\n",
      " 'so wo for the total means !'\n",
      " 'place needs a favorite sale and techs in customer service .'\n",
      " 'the casino and apologetic is so good .'\n",
      " 'i wanted to like this place but it just seeing a big anyway .'\n",
      " 'great family owned restaurant !'\n",
      " 'i would not recommend him for your yet as a ranch practice do .'\n",
      " 'love the option classes too .' 'what a fantastic company !'\n",
      " 'do not stay at this hotel .'\n",
      " \"thank , it 's all '' who were not even taken happy to her .\"\n",
      " 'the strawberry frame was too sweet too .'\n",
      " 'the assistant techs valley and 8am .'\n",
      " 'we had been answering times before and always enjoyed our food .'\n",
      " 'the food left much to be desired .'\n",
      " 'my bueno blackened is so worth it !'\n",
      " 'i wanted a regular autozone and the pool did a great job .'\n",
      " \"she did n't offer anything to connect for the battery on her entree .\"\n",
      " 'there were no thick jewelers & no popcorn of the culinary at two east .'\n",
      " 'service was fine .' 'nothing special .' 'great fun for all !'\n",
      " 'that was a interesting plus of time and money .'\n",
      " 'the other techs seem great .'\n",
      " 'but with two servers for the entire place , what did we say ?'\n",
      " 'go and get a seat outside !'\n",
      " 'outstanding customer service and dry are the best .'\n",
      " 'after son palace eye i used to discount tires on chandler sing .'\n",
      " 'take the food to go !'\n",
      " 'it looked and tasted like it came from a veggies .'\n",
      " 'plenty wash makes it a great place to get things done too .'\n",
      " \"two time i 've put in _num_ has been worse though .\"\n",
      " 'the food is not worth overwhelming - nicely sing the saturdays jewelers .'\n",
      " \"the palace at making fresh system ' was bitter .\"\n",
      " 'i brought my car back to look at it .'\n",
      " 'the doctors salad is the best around .' \"and she 's so talented !\"\n",
      " 'really friendly staff with a particularly clean place .'\n",
      " 'no beautifully in your mouth quality .' 'highly recommend .'\n",
      " 'the service was great !' 'the water was bad .' 'service was good .'\n",
      " 'these guys are great !' 'Neutral'\n",
      " 'been coming here for years , everyone is always nice & friendly .'\n",
      " \"i 'm chance to say that super incredibly `` seeing off '' .\"]\n"
     ]
    }
   ],
   "source": [
    "# Computes BLEU\n",
    "samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "print(\"samples: \",hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference:  [[\"ca n't wait to come back to watch some games and eat amazing food !\"]\n",
      " ['very pleasant experience .']\n",
      " ['i was extremely pleased with my experience .']\n",
      " ['although the treatments are a little expensive , it is certainly worth it .']\n",
      " ['will definitely be back !']\n",
      " ['no , no , no .']\n",
      " ['not anymore .']\n",
      " ['wow !']\n",
      " ['i am so not lying !']\n",
      " ['its sweet with that brown sugar topping .']\n",
      " [\"i 've eaten a lot of it in my day .\"]\n",
      " ['our bartender was very nice .']\n",
      " ['so totally not worth it .']\n",
      " ['no problem with the food here , just the service .']\n",
      " ['service was really slow .']\n",
      " ['if i could give it a negative star i would in a heartbeat !']\n",
      " ['i have called the owner several times with no return phone call .']\n",
      " ['i live and chandler and found a gem for the kids to enjoy .']\n",
      " ['so excited for the year ahead !']\n",
      " ['place needs a deep cleaning and training in customer service .']\n",
      " ['the pepperoni and ricotta is so good .']\n",
      " ['i wanted to like this place but it just became a big disappointment .']\n",
      " ['great family owned restaurant !']\n",
      " ['i would not recommend him for your use as a general practice do .']\n",
      " ['love the cooking classes too .']\n",
      " ['what a fantastic company !']\n",
      " ['do not stay at this hotel .']\n",
      " [\"hello , it 's all women who were not even paying attention to her .\"]\n",
      " ['the mango lassi was too sweet too .']\n",
      " ['the stunning mountain views and landscaping .']\n",
      " ['we had been numerous times before and always enjoyed our food .']\n",
      " ['the food left much to be desired .']\n",
      " ['my aaa membership is so worth it !']\n",
      " ['i wanted a regular mani/pedi and the girl did a great job .']\n",
      " [\"she did n't offer anything to compensate for the error on her part .\"]\n",
      " ['there were no ball washers & no pictures of the fairways at each tee .']\n",
      " ['service was fine .']\n",
      " ['nothing special .']\n",
      " ['great fun for all !']\n",
      " ['that was a complete waste of time and money .']\n",
      " ['the other techs seem great .']\n",
      " ['but with two servers for the entire place , what did we expect ?']\n",
      " ['go and get a seat outside !']\n",
      " ['outstanding customer service and products are the best .']\n",
      " ['after leaving pep boys i drove to discount tires on chandler blvd .']\n",
      " ['take the food to go !']\n",
      " ['it looked and tasted like it came from a box .']\n",
      " ['free wifi makes it a great place to get things done too .']\n",
      " [\"each time i 've returned in _num_ has been worse though .\"]\n",
      " ['the food is not worth mentioning - except perhaps the hush puppies .']\n",
      " [\"the attempt at making fresh guac ' was pitiful .\"]\n",
      " ['i brought my car back to look at it .']\n",
      " ['the chopped salad is the best around .']\n",
      " [\"and she 's so talented !\"]\n",
      " ['really friendly staff with a fairly clean place .']\n",
      " ['no melt in your mouth quality .']\n",
      " ['highly recommend .']\n",
      " ['the service was great !']\n",
      " ['the water was bad .']\n",
      " ['service was good .']\n",
      " ['these guys are great !']\n",
      " ['Neutral']\n",
      " ['been coming here for years , everyone is always nice & friendly .']\n",
      " [\"i 'm sad to say that super burrito `` fell off '' .\"]]\n"
     ]
    }
   ],
   "source": [
    "refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "refs = np.expand_dims(refs, axis=1)\n",
    "print(\"reference: \",refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_g': 15.633689880371094,\n",
       " 'loss_g_ae': 15.633689880371094,\n",
       " 'loss_g_clas': 0.6949148774147034,\n",
       " 'loss_d': 0.6888538599014282,\n",
       " 'accu_d': 0.5625,\n",
       " 'accu_g': 0.4375,\n",
       " 'accu_g_gdy': 0.4375,\n",
       " 'bleu': 0.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "vals['bleu'] = bleu\n",
    "\n",
    "avg_meters.add(vals, weight=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        try:\n",
    "            feed_dict = {\n",
    "                iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                gamma: gamma_,\n",
    "                lambda_g: lambda_g_,\n",
    "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "            }\n",
    "\n",
    "            vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n",
    "\n",
    "            batch_size = vals.pop('batch_size')\n",
    "\n",
    "            # Computes BLEU\n",
    "            samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "            hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "            print(\"samples: \",hyps)\n",
    "\n",
    "            refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "            refs = np.expand_dims(refs, axis=1)\n",
    "            print(\"reference: \",refs)\n",
    "\n",
    "            bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "            vals['bleu'] = bleu\n",
    "\n",
    "            avg_meters.add(vals, weight=batch_size)\n",
    "\n",
    "            ###################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # Writes samples\n",
    "            tx.utils.write_paired_text(\n",
    "                refs.squeeze(), hyps,\n",
    "                os.path.join(config.sample_path, 'val.%d'%epoch),\n",
    "                append=True, mode='v')\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('{}: {}'.format(\n",
    "                val_or_test, avg_meters.to_str(precision=4)))\n",
    "            break\n",
    "\n",
    "    return avg_meters.avg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.restore:\n",
    "    print('Restore from: {}'.format(config.restore))\n",
    "    saver.restore(sess, config.restore)\n",
    "\n",
    "for epoch in range(1, config.max_nepochs+1):\n",
    "    if epoch > config.pretrain_nepochs:\n",
    "        # Anneals the gumbel-softmax temperature\n",
    "        gamma_ = max(0.001, gamma_ * config.gamma_decay)\n",
    "        lambda_g_ = config.lambda_g\n",
    "    print('gamma: {}, lambda_g: {}'.format(gamma_, lambda_g_))\n",
    "\n",
    "    # Train\n",
    "    iterator.restart_dataset(sess, ['train_g', 'train_d'])\n",
    "    _train_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "    # Val\n",
    "    iterator.restart_dataset(sess, 'val')\n",
    "    _eval_epoch(sess, gamma_, lambda_g_, epoch, 'val')\n",
    "\n",
    "    saver.save(\n",
    "        sess, os.path.join(config.checkpoint_path, 'ckpt'), epoch)\n",
    "\n",
    "    # Test\n",
    "    iterator.restart_dataset(sess, 'test')\n",
    "    _eval_epoch(sess, gamma_, lambda_g_, epoch, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:save/model-1 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'save/model-1'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, os.path.join('save', 'model'), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save2/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, 'save2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:save2/model.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'save2/model.ckpt'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess,'save2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main(_):\n",
    "    # Data\n",
    "    train_data = tx.data.MultiAlignedData(config.train_data)\n",
    "    val_data = tx.data.MultiAlignedData(config.val_data)\n",
    "    test_data = tx.data.MultiAlignedData(config.test_data)\n",
    "    vocab = train_data.vocab(0)\n",
    "\n",
    "    # Each training batch is used twice: once for updating the generator and\n",
    "    # once for updating the discriminator. Feedable data iterator is used for\n",
    "    # such case.\n",
    "    iterator = tx.data.FeedableDataIterator(\n",
    "        {'train_g': train_data, 'train_d': train_data,\n",
    "         'val': val_data, 'test': test_data})\n",
    "    batch = iterator.get_next()\n",
    "\n",
    "    # Model\n",
    "    gamma = tf.placeholder(dtype=tf.float32, shape=[], name='gamma')\n",
    "    lambda_g = tf.placeholder(dtype=tf.float32, shape=[], name='lambda_g')\n",
    "    model = CtrlGenModel(batch, vocab, gamma, lambda_g, config.model)\n",
    "\n",
    "    def _train_epoch(sess, gamma_, lambda_g_, epoch, verbose=True):\n",
    "        avg_meters_d = tx.utils.AverageRecorder(size=10)\n",
    "        avg_meters_g = tx.utils.AverageRecorder(size=10)\n",
    "\n",
    "        step = 0\n",
    "        while True:\n",
    "            try:\n",
    "                step += 1\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, 'train_d'),\n",
    "                    gamma: gamma_,\n",
    "                    lambda_g: lambda_g_\n",
    "                }\n",
    "\n",
    "                vals_d = sess.run(model.fetches_train_d, feed_dict=feed_dict)\n",
    "                avg_meters_d.add(vals_d)\n",
    "\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, 'train_g'),\n",
    "                    gamma: gamma_,\n",
    "                    lambda_g: lambda_g_\n",
    "                }\n",
    "                vals_g = sess.run(model.fetches_train_g, feed_dict=feed_dict)\n",
    "                avg_meters_g.add(vals_g)\n",
    "\n",
    "                if verbose and (step == 1 or step % config.display == 0):\n",
    "                    print('step: {}, {}'.format(step, avg_meters_d.to_str(4)))\n",
    "                    print('step: {}, {}'.format(step, avg_meters_g.to_str(4)))\n",
    "\n",
    "                if verbose and step % config.display_eval == 0:\n",
    "                    iterator.restart_dataset(sess, 'val')\n",
    "                    _eval_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('epoch: {}, {}'.format(epoch, avg_meters_d.to_str(4)))\n",
    "                print('epoch: {}, {}'.format(epoch, avg_meters_g.to_str(4)))\n",
    "                break\n",
    "\n",
    "    def _eval_epoch(sess, gamma_, lambda_g_, epoch, val_or_test='val'):\n",
    "        avg_meters = tx.utils.AverageRecorder()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, val_or_test),\n",
    "                    gamma: gamma_,\n",
    "                    lambda_g: lambda_g_,\n",
    "                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n",
    "                }\n",
    "\n",
    "                vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n",
    "\n",
    "                batch_size = vals.pop('batch_size')\n",
    "\n",
    "                # Computes BLEU\n",
    "                samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n",
    "                hyps = tx.utils.map_ids_to_strs(samples['transferred'], vocab)\n",
    "                print(\"samples: \",hyps)\n",
    "\n",
    "                refs = tx.utils.map_ids_to_strs(samples['original'], vocab)\n",
    "                refs = np.expand_dims(refs, axis=1)\n",
    "                print(\"reference: \",refs)\n",
    "\n",
    "                bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n",
    "                vals['bleu'] = bleu\n",
    "\n",
    "                avg_meters.add(vals, weight=batch_size)\n",
    "\n",
    "                ###################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                # Writes samples\n",
    "                tx.utils.write_paired_text(\n",
    "                    refs.squeeze(), hyps,\n",
    "                    os.path.join(config.sample_path, 'val.%d'%epoch),\n",
    "                    append=True, mode='v')\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('{}: {}'.format(\n",
    "                    val_or_test, avg_meters.to_str(precision=4)))\n",
    "                break\n",
    "\n",
    "        return avg_meters.avg()\n",
    "\n",
    "    tf.gfile.MakeDirs(config.sample_path)\n",
    "    tf.gfile.MakeDirs(config.checkpoint_path)\n",
    "\n",
    "    # Runs the logics\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        if config.restore:\n",
    "            print('Restore from: {}'.format(config.restore))\n",
    "            saver.restore(sess, config.restore)\n",
    "\n",
    "        iterator.initialize_dataset(sess)\n",
    "\n",
    "        gamma_ = 1.\n",
    "        lambda_g_ = 0.\n",
    "        for epoch in range(1, config.max_nepochs+1):\n",
    "            if epoch > config.pretrain_nepochs:\n",
    "                # Anneals the gumbel-softmax temperature\n",
    "                gamma_ = max(0.001, gamma_ * config.gamma_decay)\n",
    "                lambda_g_ = config.lambda_g\n",
    "            print('gamma: {}, lambda_g: {}'.format(gamma_, lambda_g_))\n",
    "\n",
    "            # Train\n",
    "            iterator.restart_dataset(sess, ['train_g', 'train_d'])\n",
    "            _train_epoch(sess, gamma_, lambda_g_, epoch)\n",
    "\n",
    "            # Val\n",
    "            iterator.restart_dataset(sess, 'val')\n",
    "            _eval_epoch(sess, gamma_, lambda_g_, epoch, 'val')\n",
    "\n",
    "            saver.save(\n",
    "                sess, os.path.join(config.checkpoint_path, 'ckpt'), epoch)\n",
    "\n",
    "            # Test\n",
    "            iterator.restart_dataset(sess, 'test')\n",
    "            _eval_epoch(sess, gamma_, lambda_g_, epoch, 'test')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main=_main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
